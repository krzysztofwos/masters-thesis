<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-befe23ebd2f54d8af2c8a89d1a1611f1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.34">

  <meta name="author" content="Krzysztof Woś">
  <meta name="dcterms.date" content="2026-01-23">
  <title>MENACE as a Bayesian Observer</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-435f87ca30fd505a0374d1b057c899e8.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">MENACE as a Bayesian Observer</h1>
  <p class="subtitle">A Technical Analysis through the Free Energy Principle</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Krzysztof Woś 
</div>
        <p class="quarto-title-affiliation">
            Nakayama Laboratory
          </p>
    </div>
</div>

  <p class="date">2026-01-23</p>
</section>
<section id="michies-question-1966" class="slide level2">
<h2>Michie’s Question (1966)</h2>
<div class="columns">
<div class="column" style="width: 70%;">
<blockquote>
<p>“In simple games for which individual storage of all past board positions is feasible, is any optimal learning algorithm known? … The difficulty lies in <em>costing the acquisition of information for future use at the expense of present expected gain</em>.”</p>
<p>— Donald Michie, “Game-Playing and Game-Learning Automata” (1966)</p>
</blockquote>
</div><div class="column" style="width: 30%; text-align: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Donald-Michie-2003.jpg"></p>
<figcaption>Donald Michie</figcaption>
</figure>
</div>
</div></div>
<p><span class="highlight">Expected free energy provides a Bayes-optimal formalization of the trade-off Michie identified.</span></p>
<aside class="notes">
<p>Donald Michie was a British AI pioneer who worked alongside Alan Turing at Bletchley Park.</p>
<p>In 1961, he built MENACE — a physical machine that learned to play Tic-Tac-Toe using matchboxes and colored beads.</p>
<p>By 1966, he was asking a deeper question: given that we can store all board positions, what is the optimal way to learn?</p>
<p>The difficulty, as he put it, lies in pricing information — how much immediate performance should an agent sacrifice to gather knowledge that might be useful later? This is the exploration-exploitation trade-off stated in its original form.</p>
<p>My thesis argues that expected free energy — an objective function from Bayesian inference — provides the formal answer Michie sought.</p>
<p>I will show how MENACE implements a special case of this objective and present empirical evidence supporting this correspondence.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="thesis-contributions" class="slide level2">
<h2>Thesis Contributions</h2>
<ol type="1">
<li>A mapping from MENACE to Active Inference in Dirichlet-categorical terms</li>
<li>Identification of MENACE as an instrumental Active Inference agent</li>
<li>Empirical comparison with Active Inference variants and Reinforcement Learning (RL) baselines</li>
<li>A generative model in which <span class="math inline">\(\lambda\)</span> parameterizes the cost of information acquisition</li>
</ol>
<aside class="notes">
<p>My thesis makes four contributions.</p>
<p>First, a precise mapping from MENACE’s mechanics to Active Inference using Dirichlet-categorical distributions.</p>
<p>Second, the identification that MENACE approximately corresponds to the purely instrumental special case of expected free energy — an agent that exploits without explicit information-seeking.</p>
<p>Third, empirical validation comparing MENACE against Active Inference variants and Reinforcement Learning baselines.</p>
<p>Fourth, a generative model within the Active Inference framework that explicitly parameterizes the cost of information acquisition which directly answers Michie’s question by providing a formal mechanism to price information in the same units as outcomes.</p>
<p>I will now present the background on MENACE before developing this correspondence.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section>
<section id="part-i-background" class="title-slide slide level1 center">
<h1>Part I: Background</h1>

</section>
<section id="what-is-menace" class="slide level2">
<h2>What is MENACE?</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><span class="highlight">Physical Components</span></p>
<ul>
<li>287 matchboxes (one per board position requiring a decision)</li>
<li>Colored beads (9 colors for 9 board positions)</li>
<li>Each bead represents a possible move</li>
</ul>
<p><span class="highlight">Learning Mechanism</span></p>
<ul>
<li>Draw a bead → make that move</li>
<li>Win: add 3 beads of that color</li>
<li>Draw: add 1 bead</li>
<li>Loss: remove 1 bead</li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="assets/menace-machine.jpg"></p>
</div></div>
<p><span class="highlight">The +3/+1/−1 values encode preferences over outcomes → map to Active Inference priors.</span></p>
<aside class="notes">
<p>MENACE comprises 287 matchboxes, each representing a canonical board position where it is the X’s turn to move and a genuine decision must be made.</p>
<p>Each matchbox contains colored beads — nine colors corresponding to the nine board positions. To select a move, a bead is drawn at random. The color indicates which square to play.</p>
<p>The learning mechanism is simple: after each game, the operator revisits every matchbox that was used. If MENACE wins, three beads of the drawn color are added. If the game is a draw, one bead is added. If MENACE loses, one bead of that color is removed.</p>
<p>The +3/+1/−1 reinforcement schedule is not arbitrary — these numbers encode preferences over outcomes. They determine how quickly the system converges and how it trades off different outcomes. As we will see, these preferences map directly to prior preferences in Active Inference.</p>
<p>This physical transparency is precisely what makes MENACE valuable for theoretical analysis. Unlike neural networks or complex algorithms where the learning dynamics are opaque, every aspect of MENACE’s state is directly observable. We can examine each matchbox, count every bead, and trace exactly how experience shapes behavior.</p>
<p>This mechanism has a natural Bayesian interpretation that forms the core of my thesis. The bead counts are Dirichlet concentration parameters, and drawing a bead uniformly implements posterior predictive probability matching.</p>
<p>I will develop this connection in Part II.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="reducing-complexity-through-symmetry" class="slide level2">
<h2>Reducing Complexity Through Symmetry</h2>
<div class="columns">
<div class="column" style="width:55%;">
<p><span class="highlight">Without Symmetry</span></p>
<ul>
<li>5,478 distinct legal positions</li>
<li>Impractical to build</li>
</ul>
<p><span class="highlight">With Symmetry</span></p>
<ul>
<li>8 symmetries (4 rotations × 2 reflections)</li>
<li>765 canonical positions after symmetry reduction</li>
<li>338 X-to-move → 304 (prune forced) → 287 (prune double-threats)</li>
<li>Manageable physical system</li>
</ul>
</div><div class="column" style="width:45%;">
<p><img data-src="assets/symmetry-diagram.png" style="width:100.0%"></p>
</div></div>
<aside class="notes">
<p>Symmetry reduction is essential for making MENACE physically constructible.</p>
<p>Without it, there are thousands distinct legal board positions — far too many for a physical system. The Tic-Tac-Toe board has eight symmetries: four rotations and four reflections, forming the D4 dihedral group. Positions that differ only by symmetry are strategically equivalent, so they can share a single matchbox.</p>
<p>After symmetry reduction, these 5,478 positions collapse to just 765 canonical positions. Restricting to X-to-move positions gives 338 states. Pruning forced moves — positions where only one legal move exists — yields 304 states.</p>
<p>Michie’s original design further removes 17 double-threat positions where the outcome is already determined, yielding the final 287 states — the minimal representation of genuine decision points in the game.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="part-ii-the-correspondence" class="title-slide slide level1 center">
<h1>Part II: The Correspondence</h1>

</section>
<section id="menace-implements-dirichlet-categorical-inference" class="slide level2">
<h2>MENACE Implements Dirichlet-Categorical Inference</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="highlight">Matchbox State</span></p>
<p><span class="math display">\[\theta_s \sim \text{Dir}(\alpha_s)\]</span></p>
<p>where <span class="math inline">\(\alpha_s\)</span> = bead counts</p>
</div><div class="column" style="width:50%;">
<p><span class="highlight">Action Selection</span></p>
<ol type="1">
<li>Bead counts → move probabilities (Dirichlet posterior predictive)</li>
<li>Uniform bead draw = sample from <span class="math inline">\(\text{Cat}(\alpha_s/\alpha_{s,0})\)</span></li>
</ol>
</div></div>
<p><span class="highlight">Posterior Predictive Probability Matching</span></p>
<p>Drawing a bead uniformly = sampling from the posterior predictive</p>
<p><span class="math display">\[P(a|s) = E[\theta_{s,a}] = \frac{\alpha_{s,a}}{\alpha_{s,0}}\]</span></p>
<p><span class="highlight">Thesis contribution:</span> This mapping formalizes MENACE’s implicit Bayesian structure.</p>
<div class="legend">
<p><span class="math inline">\(\theta_s\)</span>: move probability vector for state <span class="math inline">\(s\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\alpha_s\)</span>: bead counts (Dirichlet parameters) &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\alpha_{s,0} = \sum_a \alpha_{s,a}\)</span>: total beads &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\text{Dir}\)</span>: Dirichlet distribution &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\text{Cat}\)</span>: categorical distribution</p>
</div>
<aside class="notes">
<p>This slide establishes the core mathematical correspondence.</p>
<p>Each matchbox can be viewed as maintaining a Dirichlet distribution over move probabilities, where the bead counts are the concentration parameters <span class="math inline">\(\alpha\)</span>.</p>
<p>What does it mean to draw a bead uniformly from the matchbox? In Bayesian terms, this is sampling from the posterior predictive distribution.</p>
<p>The posterior predictive answers: “Given everything I’ve learned, what action should I take?” It integrates over all possible parameter values, weighted by how plausible they are.</p>
<p>For the Dirichlet-categorical pair, this has a simple form: the probability of choosing action <span class="math inline">\(a\)</span> is just the proportion of beads — <span class="math inline">\(\alpha_a / \alpha_0\)</span>. This is called probability matching: the agent’s action probabilities directly reflect its learned beliefs.</p>
<p>This is distinct from Thompson sampling — the standard Bayesian approach in bandit problems — which would draw a full parameter vector <span class="math inline">\(\theta\)</span> from the Dirichlet and then take the argmax.</p>
<p>Probability matching naturally produces mixed strategies: actions are selected proportionally to their weights rather than always choosing the highest-weighted action. This matters in adversarial settings — a deterministic policy is predictable, allowing opponents to learn and counter it. Mixed strategies maintain unpredictability, which is why Nash equilibria in many games involve randomization.</p>
<p>The Dirichlet-categorical pairing is significant because it is a conjugate pair: the posterior has the same functional form as the prior, just with updated parameters. This conjugacy is what makes MENACE’s bead arithmetic a valid form of Bayesian updating — at least for positive reinforcement. Loss updates, which remove beads, are a heuristic rather than a literal Bayesian update.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="bayesian-updates-through-bead-manipulation" class="slide level2">
<h2>Bayesian Updates Through Bead Manipulation</h2>
<p>Standard Bayesian update <span class="math display">\[\text{Prior} + \text{Data} = \text{Posterior}\]</span></p>
<p>In Dirichlet terms <span class="math display">\[\text{Dir}(\alpha) + \text{observation} = \text{Dir}(\alpha + 1)\]</span></p>
<p>MENACE’s version <span class="math display">\[\alpha_{s,a} \leftarrow \alpha_{s,a} + U(o)\]</span></p>
<p>where <span class="math inline">\(U(o) \in \{3, 1, -1\}\)</span> is a utility-weighted pseudo-count update.</p>
<p>Loss updates are a heuristic penalty/forgetting step, not a literal conjugate posterior.</p>
<aside class="notes">
<p>This slide makes precise the sense in which MENACE’s updates are “quasi-Bayesian.” Standard Dirichlet-categorical conjugacy says that observing an outcome increments the corresponding count by one. MENACE’s updates differ in two ways.</p>
<p>First, the increments are weighted by utility: +3 for wins, +1 for draws. This is not standard Bayesian updating, but it can be interpreted as utility-weighted pseudo-count reinforcement — the agent effectively treats a win as stronger evidence than a draw.</p>
<p>Second, and more importantly, loss updates remove beads. Strict Bayesian updating only adds evidence. There is no conjugate operation for “unlearning.” The bead removal is a heuristic penalty that shifts probability away from moves that led to losses. Restocking — adding beads back when a matchbox empties — prevents the system from becoming stuck.</p>
<p>My thesis characterizes this as “directionally aligned” with reducing instrumental risk: the updates push the policy toward preferred outcomes, even if they are not exact gradient steps of a variational objective. This qualification is important for the theoretical claims.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-free-energy-principle-and-active-inference" class="slide level2">
<h2>The Free Energy Principle and Active Inference</h2>
<p><span class="highlight">Core idea:</span> Adaptive systems minimize surprise — but surprise is intractable.</p>
<p><span class="math display">\[\text{Surprise} = -\ln p(o) \quad \text{(requires marginalizing over hidden states)}\]</span></p>
<p><span class="highlight">Solution:</span> Minimize free energy <span class="math inline">\(F\)</span>, a computable upper bound on surprise.</p>
<p><span class="math display">\[F = \underbrace{D_{KL}[q(s|o) \| p(s|o)]}_{\geq 0} + \underbrace{(-\ln p(o))}_{\text{surprise}} \geq \text{surprise}\]</span></p>
<p>Active Inference operationalizes the FEP: update beliefs to match observations, or act to make observations match beliefs.</p>
<div class="legend">
<p><span class="math inline">\(F\)</span>: free energy &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(D_{KL}\)</span>: KL divergence &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(q(s|o)\)</span>: approximate posterior &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(s|o)\)</span>: true posterior &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(o)\)</span>: evidence</p>
</div>
<aside class="notes">
<p>The Free Energy Principle proposes that adaptive systems — biological or artificial — can be understood as minimizing surprise about their observations. Surprise is defined as negative log probability: <span class="math inline">\(-\ln p(o)\)</span>.</p>
<p>But there is a computational problem: computing surprise directly requires marginalizing over all hidden states, which is intractable. This is why we use free energy instead.</p>
<p>Free energy equals KL divergence plus surprise. Since KL divergence is non-negative, free energy is always an upper bound on surprise. Minimizing free energy does two things: it makes the approximate posterior closer to the true posterior, and it tightens the bound on surprise.</p>
<p>Active Inference operationalizes the Free Energy Principle. It provides a unified framework for both perception and action: agents can minimize surprise by updating their beliefs to better predict observations (perception), or by acting to generate observations they expect (action).</p>
<p>MENACE learns policies — it changes its action selection — to make winning unsurprising. This is the key connection to Active Inference that we will develop.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="expected-free-energy-with-λ-parameter" class="slide level2">
<h2>Expected Free Energy with λ Parameter</h2>
<p>The epistemic weight <span class="math inline">\(\lambda\)</span> prices information in outcome units:</p>
<p><span class="math display">\[G_\lambda(\pi) = \underbrace{\text{Risk}(\pi)}_{\text{instrumental cost}} - \lambda \underbrace{I(o;\theta)}_{\text{epistemic value}}\]</span></p>
<ul>
<li>Risk: <span class="math inline">\(D_{KL}(q(o|\pi) \| p(o|C))\)</span> — distance from preferred outcomes</li>
<li>Epistemic Value: <span class="math inline">\(I(o;\theta)\)</span> — expected information gain about parameters</li>
</ul>
<p><span class="math inline">\(\lambda\)</span> is Michie’s “exchange rate” between information and immediate gain.</p>
<p><span class="highlight">Decision objective:</span> Expected free energy makes the utility + information trade-off explicit.</p>
<div class="legend">
<p><span class="math inline">\(G_\lambda(\pi)\)</span>: expected free energy &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\pi\)</span>: policy &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\lambda\)</span>: epistemic weight &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(o\)</span>: outcome &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\theta\)</span>: model parameters &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(I(o;\theta)\)</span>: information gain about <span class="math inline">\(\theta\)</span> from observing <span class="math inline">\(o\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(q(o|\pi)\)</span>: predicted outcomes &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(C\)</span>: prior preferences &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(o|C)\)</span>: preferred outcomes</p>
</div>
<aside class="notes">
<p>Variational free energy handles inference about the present. For planning — choosing actions under uncertainty — Active Inference extends this to expected free energy, which evaluates policies by their anticipated consequences.</p>
<p>The expected free energy <span class="math inline">\(G\)</span> parameterized by <span class="math inline">\(\lambda\)</span> for policy <span class="math inline">\(\pi\)</span> equals the risk under that policy minus <span class="math inline">\(\lambda\)</span> times the information gain about parameters <span class="math inline">\(\theta\)</span> from observing an outcome <span class="math inline">\(o\)</span>.</p>
<p>The decomposition shown here separates two components:</p>
<ul>
<li>Risk is the KL divergence between the predicted outcome distribution and the agent’s preferences. It measures how far expected outcomes are from desired outcomes.</li>
<li>Epistemic value is the mutual information between observations and model parameters. It measures how much the agent expects to learn.</li>
</ul>
<p>The <span class="math inline">\(\lambda\)</span> parameter weights the epistemic term.</p>
<p>When <span class="math inline">\(\lambda\)</span> equals zero, the agent pursues only instrumental goals — minimizing risk, maximizing preferred outcomes.</p>
<p>When <span class="math inline">\(\lambda\)</span> is positive, the agent trades off immediate performance against information gain.</p>
<p>This is precisely the exchange rate Michie requested. The <span class="math inline">\(\lambda\)</span> parameter prices “information for future use” in the same units as “present expected gain.” Different values of <span class="math inline">\(\lambda\)</span> yield different exploration-exploitation trade-offs, and my thesis tests this empirically.</p>
<p>Note that the epistemic term enters with a negative sign because we minimize <span class="math inline">\(G\)</span> — more information gain reduces expected free energy, making information-seeking policies more attractive under minimization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="menaces-implicit-generative-model" class="slide level2">
<h2>MENACE’s Implicit Generative Model</h2>
<p>What MENACE “believes”</p>
<p><span class="math display">\[p(o, s_{0:T}, a_{0:T}) = p(o|s_T) \prod_{t} p(s_{t+1}|s_t, a_t) p(a_t|s_t)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(p(o|s_T)\)</span>: Outcome model — final position determines outcome (deterministic)</li>
<li><span class="math inline">\(p(s_{t+1}|s_t, a_t)\)</span>: Transition model — game rules (fixed)</li>
<li><span class="math inline">\(p(a_t|s_t)\)</span>: Policy to learn (the beads)</li>
</ul>
<p>All of MENACE’s learning focuses on optimizing the policy component.</p>
<div class="legend">
<p><span class="math inline">\(o\)</span>: outcome &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(s_t\)</span>: state at time <span class="math inline">\(t\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(a_t\)</span>: action at time <span class="math inline">\(t\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(T\)</span>: terminal time &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(o|s_T)\)</span>: outcome model &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(s_{t+1}|s_t,a_t)\)</span>: transition model &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(a_t|s_t)\)</span>: policy</p>
</div>
<aside class="notes">
<p>To apply Active Inference to MENACE, we must specify its implicit generative model — the probabilistic model of how games unfold. This factorization has three components.</p>
<p>First, the outcome model <span class="math inline">\(p(o|s_T)\)</span>: terminal board positions deterministically map to outcomes. MENACE encodes this implicitly — reinforcement is applied based on game results, so the system “knows” which positions are winning, losing, or drawing.</p>
<p>Second, the transition model <span class="math inline">\(p(s_{t+1}|s_t, a_t)\)</span>: the rules of Tic-Tac-Toe. These are fixed and encoded in how the human operator uses the system — the operator opens the matchbox corresponding to the current board state, so transitions are handled externally.</p>
<p>Third, the policy <span class="math inline">\(p(a_t|s_t)\)</span>: this is what the beads represent. Each matchbox encodes MENACE’s beliefs about which moves are preferable in that position.</p>
<p>All of MENACE’s learning focuses on the policy component. The game rules and outcome mappings are fixed. Only the action preferences change.</p>
<p>This factorization is the same structure used by modern reinforcement learning algorithms, but implemented in physical hardware.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="preference-weighted-policy-shaping" class="slide level2">
<h2>Preference-Weighted Policy Shaping</h2>
<p>MENACE corresponds to the instrumental objective</p>
<p><span class="math display">\[G_\lambda(\pi) = \mathrm{Risk}(\pi) - \lambda I(o;\theta), \quad \lambda = 0\]</span></p>
<ul>
<li>Preferences are encoded by <span class="math inline">\(U(o) \in \{+3,+1,-1\}\)</span></li>
<li>Bead updates shift Dirichlet counts toward preferred outcomes</li>
<li>Negative updates are heuristic penalties; restocking keeps <span class="math inline">\(\alpha_{s,a} &gt; 0\)</span></li>
</ul>
<p>The update is directionally aligned with reducing instrumental risk, not an exact gradient step.</p>
<p><span class="highlight">Thesis contribution:</span> This model is instantiated for MENACE and Tic-Tac-Toe, with explicit computation of all quantities.</p>
<div class="legend">
<p><span class="math inline">\(G_\lambda(\pi)\)</span>: expected free energy &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\pi\)</span>: policy &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(o\)</span>: outcome &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\theta\)</span>: model parameters (bead proportions) &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(I(o;\theta)\)</span>: information gain about <span class="math inline">\(\theta\)</span> from <span class="math inline">\(o\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(U(o)\)</span>: utility &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\alpha_{s,a}\)</span>: bead count</p>
</div>
<aside class="notes">
<p><strong>Reading the equation:</strong> “The expected free energy G sub lambda for policy pi equals the risk under that policy minus lambda times information gain, where lambda equals zero — so only the risk term remains.”</p>
<p>This slide states the central theoretical claim. MENACE corresponds to the instrumental special case of expected free energy minimization — that is, <span class="math inline">\(\lambda\)</span> equals zero, with no explicit epistemic drive.</p>
<p>The preferences encoded by the reinforcement schedule (+3/+1/-1) map to the prior preference distribution <span class="math inline">\(p(o|C)\)</span> in the Active Inference formulation. Bead updates shift Dirichlet parameters toward outcomes with higher utility, which is directionally aligned with reducing the risk term — the KL divergence between predicted and preferred outcomes.</p>
<p>Three important caveats qualify this claim.</p>
<p>First, the update is directionally aligned with reducing risk, not an exact gradient step of a variational objective.</p>
<p>Second, loss updates (removing beads) are heuristic rather than conjugate Bayesian updates.</p>
<p>Third, restocking — adding beads back when matchboxes empty — is a practical mechanism without a clean theoretical interpretation.</p>
<p>Despite these caveats, my thesis contribution is to make this correspondence concrete: I instantiate this Active Inference model for MENACE and Tic-Tac-Toe, computing all quantities explicitly. This allows empirical testing of whether the theoretical correspondence holds in practice.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="part-iii-empirical-analysis" class="title-slide slide level1 center">
<h1>Part III: Empirical Analysis</h1>

</section>
<section id="experimental-setup" class="slide level2">
<h2>Experimental Setup</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="highlight">Agents Compared</span></p>
<ul>
<li>MENACE (Michie filter, box restock)</li>
<li>Instrumental AIF (<span class="math inline">\(\lambda = 0\)</span>)</li>
<li>Hybrid AIF (<span class="math inline">\(\lambda = 0.5\)</span>)</li>
<li>Pure AIF (<span class="math inline">\(\lambda \in \{0, 0.25, 0.5\}\)</span>)</li>
<li>Q-Learning / SARSA baselines</li>
</ul>
</div><div class="column" style="width:50%;">
<p><span class="highlight">Protocol</span></p>
<ul>
<li>Training: 500 games (5,000 for RL)</li>
<li>Validation: 100 games vs.&nbsp;optimal</li>
<li>Seeds: 10 independent runs</li>
<li>State filters: Michie (287 states)</li>
<li>MENACE curriculum: mixed; AIF baseline: optimal-only</li>
</ul>
</div></div>
<p><span class="highlight">Thesis contribution:</span> First systematic empirical comparison of MENACE against Active Inference variants.</p>
<aside class="notes">
<p>The theoretical correspondence makes testable predictions. If MENACE corresponds to instrumental Active Inference with <span class="math inline">\(\lambda = 0\)</span>, then an explicit Active Inference agent with <span class="math inline">\(\lambda = 0\)</span> should achieve similar performance.</p>
<p>The experiments compare five agent types:</p>
<ul>
<li>MENACE with Michie’s original 287-state filter and box-level restocking</li>
<li>Instrumental Active Inference with <span class="math inline">\(\lambda = 0\)</span></li>
<li>Hybrid Active Inference, which combines MENACE-style Dirichlet updates with EFE-based action selection</li>
<li>Pure Active Inference, which uses EFE for both learning and action selection without the Dirichlet machinery</li>
<li>Tabular Reinforcement Learning baselines: Q-learning and SARSA</li>
</ul>
<p>Both Hybrid and Pure variants are tested with positive <span class="math inline">\(\lambda\)</span> values to add explicit epistemic drive. Tabular RL baselines serve as reference points from the reinforcement learning literature.</p>
<p>SARSA (State-Action-Reward-State-Action) is on-policy temporal difference control: it updates Q-values using the action actually taken next, whereas Q-learning is off-policy and updates using the maximum over next actions.</p>
<p>All agents are trained for 500 games, except RL baselines which receive 5,000 games for asymptotic comparison. Post-training validation uses 100 games against optimal play. Results aggregate over 10 independent seeds.</p>
<p>One methodological note: MENACE uses a mixed curriculum — starting against random opponents and progressing to optimal — while the AIF baseline trains directly against optimal. This means training trajectories are not directly comparable, but post-training validation against the same opponent provides the fair comparison point.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-finding-1-instrumental-equivalence" class="slide level2">
<h2>Key Finding 1: Instrumental Equivalence</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Algorithm</th>
<th style="text-align: center;">Draw Rate (%)</th>
<th style="text-align: center;">Loss Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MENACE (box restock)</td>
<td style="text-align: center;"><span class="math inline">\(84.5 \pm 8.1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(15.5 \pm 8.1\)</span></td>
</tr>
<tr class="even">
<td>Instrumental AIF (<span class="math inline">\(\lambda=0\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(88.1 \pm 3.9\)</span></td>
<td style="text-align: center;"><span class="math inline">\(11.9 \pm 3.9\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>When <span class="math inline">\(\lambda = 0\)</span>, the EFE-based policy reduces to a purely instrumental objective, closely matching MENACE’s pseudo-count reinforcement</li>
<li>However, MENACE has implicit exploration: low concentration → high variance → naturally exploratory early behavior</li>
</ul>
<p><span class="highlight">Result:</span> MENACE ≈ Instrumental Active Inference.</p>
<aside class="notes">
<p>The first key finding tests the central theoretical claim: does MENACE correspond to instrumental Active Inference?</p>
<p>MENACE achieves 84.5% draw rate; Instrumental AIF with <span class="math inline">\(\lambda = 0\)</span> achieves 88.1%. Overlapping confidence intervals support the correspondence — both optimize approximately the same objective.</p>
<p>Residual differences come from implementation details. Note that MENACE has implicit exploration via the Dirichlet: low concentration means high variance, which diminishes as beliefs consolidate.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-finding-2-the-value-of-information" class="slide level2">
<h2>Key Finding 2: The Value of Information</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Algorithm</th>
<th style="text-align: center;">Draw Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pure AIF (<span class="math inline">\(\lambda = 0.0\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(79.7 \pm 6.5\)</span></td>
</tr>
<tr class="even">
<td>Pure AIF (<span class="math inline">\(\lambda = 0.25\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(79.1 \pm 4.8\)</span></td>
</tr>
<tr class="odd">
<td>Pure AIF (<span class="math inline">\(\lambda = 0.5\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(77.0 \pm 3.7\)</span></td>
</tr>
</tbody>
</table>
<p><span class="highlight">Paradox:</span> Epistemic variants (<span class="math inline">\(\lambda &gt; 0\)</span>) do NOT outperform instrumental baseline (<span class="math inline">\(\lambda=0\)</span>) within 500 games.</p>
<p><span class="highlight">Resolution:</span> This reflects Michie’s trade-off in practice: “acquisition of information for future use <em>at the expense of present expected gain</em>”.</p>
<aside class="notes">
<p>The second finding addresses Michie’s original question: what is the cost of information?</p>
<p>Higher <span class="math inline">\(\lambda\)</span> means lower short-term performance: 79.7% at <span class="math inline">\(\lambda = 0\)</span> down to 77.0% at <span class="math inline">\(\lambda = 0.5\)</span>. This confirms Michie’s prediction — information has value for future decisions, but acquiring it costs immediate performance.</p>
<p>This quantifies the trade-off: explicit epistemic value has a measurable short-horizon cost.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-finding-3-robustness-vs.-specialization" class="slide level2">
<h2>Key Finding 3: Robustness vs.&nbsp;Specialization</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Training</th>
<th style="text-align: center;">Draw Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q-learning</td>
<td>random</td>
<td style="text-align: center;"><span class="math inline">\(98.0 \pm \phantom{0}1.2\)</span></td>
</tr>
<tr class="even">
<td>Q-learning</td>
<td>defensive</td>
<td style="text-align: center;"><span class="math inline">\(10.2 \pm 30.9\)</span></td>
</tr>
<tr class="odd">
<td>SARSA</td>
<td>random</td>
<td style="text-align: center;"><span class="math inline">\(97.9 \pm \phantom{0}1.9\)</span></td>
</tr>
<tr class="even">
<td>SARSA</td>
<td>defensive</td>
<td style="text-align: center;"><span class="math inline">\(20.5 \pm 40.3\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Box-level restocking preserves full support</li>
<li>Provides implicit “insurance” against distributional shift</li>
<li>Q-learning can drive Q-values to extremes, zeroing out actions</li>
<li>MENACE achieves competitive performance with 10× fewer games</li>
</ul>
<p><span class="highlight">Dirichlet advantage:</span> MENACE keeps all action probabilities strictly positive.</p>
<aside class="notes">
<p>The third finding compares MENACE’s robustness against tabular RL baselines.</p>
<p>RL achieves 98% when training and evaluation match, but collapses under distribution shift — Q-learning trained on defensive drops to 10% against optimal. MENACE’s restocking ensures all actions keep positive probability, providing implicit regularization.</p>
<p>Also note sample efficiency: MENACE achieves competitive performance with 500 games versus 5,000 for RL.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="answering-michies-question" class="slide level2">
<h2>Answering Michie’s Question</h2>
<blockquote>
<p>“The difficulty lies in <em>costing the acquisition of information</em> for future use at the expense of present expected gain.”</p>
</blockquote>
<p><span class="math display">\[G_\lambda(\pi) = \text{Risk}(\pi) - \lambda \cdot I(o;\theta)\]</span></p>
<p>The scalar <span class="math inline">\(\lambda \geq 0\)</span> is the exchange rate Michie asked for — not a single algorithm, but a family:</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="highlight">Active Inference (General)</span></p>
<ul>
<li>Epistemic value priced via <span class="math inline">\(\lambda\)</span></li>
<li>Trade-off is a design parameter</li>
</ul>
</div><div class="column" style="width:50%;">
<p><span class="highlight">MENACE (Special Case)</span></p>
<ul>
<li>Instrumental objective (<span class="math inline">\(\lambda = 0\)</span>)</li>
<li>Exploration emerges from uncertainty</li>
</ul>
</div></div>
<p><span class="highlight">Thesis contribution:</span> This framework answers Michie’s question with MENACE as a concrete, mechanizable special case.</p>
<div class="legend">
<p><span class="math inline">\(G_\lambda(\pi)\)</span>: expected free energy &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\lambda\)</span>: exchange rate &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(I(o;\theta)\)</span>: information gain &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\text{Risk}\)</span>: instrumental cost</p>
</div>
<aside class="notes">
<p>Finally, we return to Michie’s original question: how to price information against immediate performance?</p>
<p>The <span class="math inline">\(\lambda\)</span> parameter is the exchange rate. When <span class="math inline">\(\lambda = 0\)</span>, information has no explicit value — the agent is purely instrumental. When <span class="math inline">\(\lambda\)</span> is positive, the agent sacrifices immediate performance to reduce uncertainty.</p>
<p>The answer is not a single optimal algorithm but a family of objectives. MENACE implements <span class="math inline">\(\lambda = 0\)</span> with implicit exploration from probability matching. Active Inference provides the general framework where the trade-off becomes an explicit design choice.</p>
<p>My thesis places MENACE within this modern framework as a mechanically interpretable special case.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="conclusions" class="slide level2">
<h2>Conclusions</h2>
<p><span class="highlight">Key Findings</span></p>
<ul>
<li>Dirichlet-categorical mapping formalizes MENACE’s Bayesian structure</li>
<li>MENACE ≈ instrumental Active Inference (<span class="math inline">\(\lambda = 0\)</span>) confirmed empirically</li>
<li><span class="math inline">\(\lambda\)</span> quantifies information cost with measurable short-horizon effects</li>
<li>Dirichlet representation provides robustness that tabular RL lacks</li>
</ul>
<p><span class="highlight">MENACE is a historically remarkable, mechanizable special case of Active Inference.</span></p>
<aside class="notes">
<p>To summarize the key findings.</p>
<p>First, the theoretical contribution: the Dirichlet-categorical mapping formalizes MENACE’s implicit Bayesian structure.</p>
<p>Second, instrumental equivalence: MENACE and an Active Inference agent with epistemic drive suppressed achieve comparable performance, supporting the identification of MENACE as the purely instrumental special case.</p>
<p>Third, the cost of information: explicit epistemic value (positive <span class="math inline">\(\lambda\)</span>) reduces short-horizon performance, exactly as Michie predicted. The <span class="math inline">\(\lambda\)</span> parameter makes this trade-off an explicit design choice.</p>
<p>Fourth, robustness: the Dirichlet representation provides natural protection against distributional shift that tabular RL lacks, because restocking keeps all actions at positive probability.</p>
<p>The takeaway is that MENACE — a sixty-year-old physical learning machine — instantiates principles that remain central to modern Bayesian approaches to learning and decision-making.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section id="thank-you-for-your-attention" class="title-slide slide level1 center">
<h1>Thank you for your attention</h1>
<p>Code, data, and thesis: <a href="https://github.com/krzysztofwos/masters-thesis">github.com/krzysztofwos/masters-thesis</a></p>
</section>

<section>
<section id="appendix" class="title-slide slide level1 center">
<h1>Appendix</h1>

</section>
<section id="a1-donald-michie-1923-2007" class="slide level2">
<h2>A1: Donald Michie (1923-2007)</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="assets/donald-michie-1.jpg" alt="Donald Michie" style="width:100%;"></p>
</div><div class="column" style="width:35%;">
<p><span class="highlight">The Pioneer Behind MENACE</span></p>
<ul>
<li>Codebreaker at Bletchley Park during WWII</li>
<li>Worked alongside Alan Turing on Colossus</li>
<li>MA, DPhil, DSc in biological sciences (Oxford)</li>
<li>Professor of Machine Intelligence, Edinburgh</li>
</ul>
<p><span class="highlight">Key insight:</span> “Programming human intelligence into machines” — inspired by wartime cryptanalysis.</p>
</div><div class="column" style="width:35%;">
<p><span class="highlight">Contributions to AI</span></p>
<ul>
<li>Founded Edinburgh’s Machine Intelligence unit</li>
<li>Editor-in-Chief, “Machine Intelligence” series</li>
<li>Developed machine learning into “industrial-strength tool”</li>
<li>1996 Feigenbaum Medal for ML applications</li>
</ul>
<p><span class="highlight">MENACE (1961):</span> Physical demonstration that learning could be mechanized.</p>
</div></div>
<aside class="notes">
<p>Donald Michie was a polymath who bridged biology, computing, and artificial intelligence. His wartime experience at Bletchley Park, working alongside Alan Turing on Colossus, convinced him that machines could exhibit intelligent behavior through systematic processes.</p>
<p>MENACE — the Machine Educable Noughts And Crosses Engine — was Michie’s 1961 demonstration that learning could emerge from simple mechanical operations: matchboxes, colored beads, and a reinforcement rule.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a2-dirichlet-categorical-conjugacy-proof" class="slide level2">
<h2>A2: Dirichlet-Categorical Conjugacy Proof</h2>
<p><span class="highlight">Theorem:</span> If <span class="math inline">\(\theta \sim \text{Dir}(\alpha)\)</span> and <span class="math inline">\(n\)</span> outcomes are observed, then: <span class="math display">\[\theta|\text{data} \sim \text{Dir}(\alpha + n)\]</span></p>
<p><span class="highlight">Proof:</span></p>
<p><span class="math display">\[
\begin{align}
p(\theta|\text{data}) &amp;\propto p(\text{data}|\theta)p(\theta) \\
&amp;= \prod_{i=1}^{k} \theta_i^{n_i} \cdot \frac{1}{B(\alpha)} \prod_{i=1}^{k} \theta_i^{\alpha_i-1} \\
&amp;\propto \prod_{i=1}^{k} \theta_i^{\alpha_i + n_i - 1}
\end{align}
\]</span></p>
<p>This is the kernel of <span class="math inline">\(\text{Dir}(\alpha + n)\)</span> ∎</p>
<aside class="notes">
<p>If <span class="math inline">\(\theta\)</span> is drawn from a Dirichlet with parameters <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(n\)</span> outcomes are observed, then <span class="math inline">\(\theta\)</span> given data is Dirichlet with parameters <span class="math inline">\(\alpha\)</span> plus <span class="math inline">\(n\)</span>.</p>
<p>This proof shows why MENACE’s bead-updating mechanism is mathematically elegant.</p>
<p>The Dirichlet-categorical conjugacy means that starting with a Dirichlet prior (initial bead counts) and observing categorical data (game outcomes), the posterior is also Dirichlet with updated parameters.</p>
<p>In MENACE’s context:</p>
<ul>
<li>The initial bead counts <span class="math inline">\(\alpha\)</span> represent prior beliefs about which moves are good</li>
<li>Each game outcome adds evidence: winning adds to the count for moves that led to victory</li>
<li>Win/draw increments are additive evidence; loss decrements are a heuristic/forgetting step</li>
<li>The posterior <span class="math inline">\(\text{Dir}(\alpha + n)\)</span> is the updated belief after incorporating this evidence</li>
</ul>
<p>The key property is that updating only requires adding integers — no complex calculations needed. This is why Michie could implement Bayesian inference with physical beads: the mathematics naturally maps to counting operations.</p>
<p>This conjugacy is not a coincidence but reflects a deep connection between counting and probability. The Dirichlet distribution is the natural prior for categorical probabilities precisely because it makes updating trivial through simple addition.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a3-posterior-predictive-probability-matching" class="slide level2">
<h2>A3: Posterior Predictive Probability Matching</h2>
<p><span class="highlight">Theorem:</span> Drawing beads = posterior predictive probability matching.</p>
<p><span class="highlight">Proof:</span> For a Dirichlet–categorical model,</p>
<p><span class="math display">\[P(a) = \int \theta_a \cdot \text{Dir}(\theta; \alpha) \, d\theta = \frac{\alpha_a}{\sum_i \alpha_i}\]</span></p>
<p>MENACE’s probability <span class="math display">\[P(a) = \frac{\text{# beads of color } a}{\text{total beads}} = \frac{\alpha_a}{\sum_i \alpha_i}\]</span></p>
<p>Identical ∎</p>
<div class="legend">
<p><span class="math inline">\(P(a)\)</span>: probability of action <span class="math inline">\(a\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\theta_a\)</span>: move probability for action <span class="math inline">\(a\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\text{Dir}(\theta; \alpha)\)</span>: Dirichlet distribution &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\alpha_a\)</span>: bead count for action <span class="math inline">\(a\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\sum_i \alpha_i\)</span>: total beads</p>
</div>
<aside class="notes">
<p>The probability of action <span class="math inline">\(a\)</span> equals the integral of <span class="math inline">\(\theta_a\)</span> times the Dirichlet density, which equals <span class="math inline">\(\alpha_a\)</span> over the sum of all alphas.</p>
<p>MENACE samples directly from the posterior predictive distribution (the Dirichlet mean). This is probability matching, not canonical Thompson sampling.</p>
<p>Thompson sampling would first sample <span class="math inline">\(\theta \sim \text{Dir}(\alpha)\)</span> and then take <span class="math inline">\(\arg\max_a \theta_a\)</span>. MENACE instead samples actions in proportion to <span class="math inline">\(\alpha/\alpha_0\)</span>, which naturally implements mixed strategies.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a4-dirichletcategorical-mutual-information" class="slide level2">
<h2>A4: Dirichlet–Categorical Mutual Information</h2>
<p>Epistemic value in the thesis is the mutual information between observations and parameters.</p>
<p><span class="math display">\[I(o;\theta) = H\!\left[\text{Cat}\!\left(\frac{\alpha}{\alpha_0}\right)\right] - \left[\psi(\alpha_0 + 1) - \sum_i \frac{\alpha_i}{\alpha_0} \psi(\alpha_i + 1)\right]\]</span></p>
<p>Equivalent form:</p>
<p><span class="math display">\[I(o;\theta) = \sum_i \frac{\alpha_i}{\alpha_0}\!\left[\psi(\alpha_i{+}1)-\psi(\alpha_0{+}1)-\ln\!\frac{\alpha_i}{\alpha_0}\right] \ge 0\]</span></p>
<p>As total concentration <span class="math inline">\(\alpha_0\)</span> becomes large, <span class="math inline">\(I(o;\theta) \to 0\)</span>.</p>
<div class="legend">
<p><span class="math inline">\(I(o;\theta)\)</span>: mutual information &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(o\)</span>: observation (action or outcome) &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\theta \sim \text{Dir}(\alpha)\)</span>: probability vector &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\alpha_i\)</span>: concentration parameter for category <span class="math inline">\(i\)</span> &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\alpha_0 = \sum_i \alpha_i\)</span>: total concentration &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(\psi\)</span>: digamma function &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(H\)</span>: entropy</p>
</div>
<aside class="notes">
<p>The mutual information between observations <span class="math inline">\(o\)</span> and parameters <span class="math inline">\(\theta\)</span> equals the entropy of the categorical distribution minus the expected conditional entropy, expressed using digamma functions <span class="math inline">\(\psi\)</span>.</p>
<p>This formula quantifies epistemic value — how much observing a sample from the categorical distribution reveals about the underlying parameters. In MENACE’s context, <span class="math inline">\(o\)</span> can be either an action selection (at the matchbox level) or a game outcome (at the trajectory level), depending on what Dirichlet is being queried.</p>
<p>The first form decomposes mutual information as entropy minus conditional entropy. The term <span class="math inline">\(H[\text{Cat}(\alpha/\alpha_0)]\)</span> is the entropy of the posterior predictive categorical distribution (uncertainty about the next observation). The bracketed term is the expected entropy of the likelihood given the parameters.</p>
<p>The equivalent form makes the non-negativity explicit. Each term in the sum is non-negative because the digamma function satisfies <span class="math inline">\(\psi(x+1) - \psi(y+1) \geq \ln(x/y)\)</span> when <span class="math inline">\(x \leq y\)</span>.</p>
<p>The key insight: as concentration <span class="math inline">\(\alpha_0\)</span> grows, mutual information shrinks toward zero. With high concentration (high confidence), the probabilities are already well-estimated — observing another sample teaches little. With low concentration (high uncertainty), each observation is highly informative.</p>
<p>This explains why epistemic value matters early in learning but diminishes over time. An agent that weights epistemic value highly will explore more when uncertain, then naturally transition to exploitation as beliefs consolidate.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a5-deriving-variational-free-energy" class="slide level2">
<h2>A5: Deriving Variational Free Energy</h2>
<p><span class="highlight">Goal:</span> Approximate intractable posterior <span class="math inline">\(p(s|o)\)</span> with tractable <span class="math inline">\(q(s|o)\)</span>.</p>
<p><span class="math display">\[
\begin{align}
D_{KL}[q(s|o) \| p(s|o)] &amp;= \mathbb{E}_q\left[\ln q(s|o) - \ln \frac{p(o|s)p(s)}{p(o)}\right] \\
&amp;= \mathbb{E}_q[\ln q(s|o) - \ln p(o|s) - \ln p(s)] + \ln p(o) \\
&amp;= \underbrace{D_{KL}[q(s|o) \| p(s)] - \mathbb{E}_q[\ln p(o|s)]}_{F \;=\; \text{Free Energy}} + \ln p(o)
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(D_{KL} \geq 0\)</span>: <span class="math inline">\(\quad F \geq -\ln p(o) = \text{Surprise}\)</span></p>
<p><span class="highlight">Key insight:</span> Minimizing <span class="math inline">\(F\)</span> simultaneously approximates the posterior and bounds surprise.</p>
<div class="legend">
<p><span class="math inline">\(q(s|o)\)</span>: approximate posterior &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(s|o)\)</span>: true posterior &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(p(o)\)</span>: evidence/marginal likelihood &nbsp;&nbsp;•&nbsp;&nbsp; <span class="math inline">\(F\)</span>: variational free energy</p>
</div>
<aside class="notes">
<p>This derivation shows how variational free energy arises from trying to minimize KL divergence between an approximate posterior and the true posterior.</p>
<p>Starting with the KL divergence definition and applying Bayes’ rule to expand <span class="math inline">\(p(s|o)\)</span>, we can rearrange terms to isolate what we call free energy.</p>
<p>The key insight is that since KL divergence is always non-negative, free energy is always an upper bound on surprise (negative log evidence). Minimizing free energy therefore accomplishes two things simultaneously: it makes the approximate posterior closer to the true posterior, and it minimizes surprise about observations.</p>
<p>This is the mathematical foundation for why Active Inference agents minimize free energy — it is a tractable proxy for minimizing surprise while performing approximate Bayesian inference.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a6-three-equivalent-forms-of-free-energy" class="slide level2">
<h2>A6: Three Equivalent Forms of Free Energy</h2>
<div class="white-math">
<table class="caption-top">
<colgroup>
<col style="width: 14%">
<col style="width: 85%">
</colgroup>
<thead>
<tr class="header">
<th>Form</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Energy − Entropy</td>
<td><span class="math inline">\(F = \underbrace{-\mathbb{E}_q[\ln p(o,s)]}_{\text{Energy}} + \underbrace{H[q(s \mid o)]}_{\text{Entropy}}\)</span></td>
</tr>
<tr class="even">
<td>Complexity − Accuracy</td>
<td><span class="math inline">\(F = \underbrace{D_{KL}[q \parallel p(s)]}_{\text{Complexity}} - \underbrace{\mathbb{E}_q[\ln p(o \mid s)]}_{\text{Accuracy}}\)</span></td>
</tr>
<tr class="odd">
<td>Divergence + Surprise</td>
<td><span class="math inline">\(F = \underbrace{D_{KL}[q \parallel p(s \mid o)]}_{\text{Divergence}} + \underbrace{(-\ln p(o))}_{\text{Surprise}}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Each form highlights a different trade-off:</p>
<ul>
<li><span class="highlight">Energy−Entropy:</span> Fit data while maintaining uncertainty</li>
<li><span class="highlight">Complexity−Accuracy:</span> Simple models that explain data well</li>
<li><span class="highlight">Divergence+Surprise:</span> Approximate inference while minimizing surprise</li>
</ul>
<aside class="notes">
<p>These three formulations of variational free energy are mathematically equivalent but emphasize different aspects of the optimization.</p>
<p>The Energy-Entropy form comes from statistical mechanics and shows the balance between fitting observations (low energy) and maintaining appropriate uncertainty (high entropy).</p>
<p>The Complexity-Accuracy form is common in machine learning. Complexity penalizes posteriors that deviate from the prior, while accuracy rewards explaining the data. This is the form used in variational autoencoders.</p>
<p>The Divergence-Surprise form most directly connects to Active Inference. It shows that free energy equals the gap between approximate and true posteriors plus surprise. When the approximate posterior equals the true posterior, free energy equals surprise exactly.</p>
<p>For MENACE and Active Inference, the Complexity-Accuracy form is most relevant: risk corresponds to the accuracy term (achieving preferred outcomes), while the Dirichlet prior provides implicit complexity regularization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: true,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/krzysztofwos\.github\.io\/masters-thesis\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script src="fit-text.js"></script>
    <script src="fit-body.js"></script>
    <script>
    // Inject notes into the title slide
    document.addEventListener('DOMContentLoaded', function() {
      const titleSlide = document.getElementById('title-slide');
      if (titleSlide) {
        const notes = document.createElement('aside');
        notes.className = 'notes';
        notes.innerHTML = `
          Thank you for the opportunity to present my work. My thesis addresses a question Donald Michie posed in 1966: how should a learning agent price information against immediate performance? I'll present a formal answer and show that MENACE — his physical learning machine — implements a special case.
        `;
        titleSlide.appendChild(notes);
      }
    });
    </script>
    

</body></html>