[
  {
    "objectID": "slides/01-menace-machine.html",
    "href": "slides/01-menace-machine.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Physical Components\n\n287 matchboxes (one per board position requiring a decision)\nColored beads (9 colors for 9 board positions)\nEach bead represents a possible move\n\nLearning Mechanism\n\nDraw a bead → make that move\nWin: add 3 beads of that color\nDraw: add 1 bead\nLoss: remove 1 bead\n\n\n\n\n\nThe +3/+1/−1 values encode preferences over outcomes → map to Active Inference priors.\n\nMENACE comprises 287 matchboxes, each representing a canonical board position where it is the X’s turn to move and a genuine decision must be made.\nEach matchbox contains colored beads — nine colors corresponding to the nine board positions. To select a move, a bead is drawn at random. The color indicates which square to play.\nThe learning mechanism is simple: after each game, the operator revisits every matchbox that was used. If MENACE wins, three beads of the drawn color are added. If the game is a draw, one bead is added. If MENACE loses, one bead of that color is removed.\nThe +3/+1/−1 reinforcement schedule is not arbitrary — these numbers encode preferences over outcomes. They determine how quickly the system converges and how it trades off different outcomes. As we will see, these preferences map directly to prior preferences in Active Inference.\nThis physical transparency is precisely what makes MENACE valuable for theoretical analysis. Unlike neural networks or complex algorithms where the learning dynamics are opaque, every aspect of MENACE’s state is directly observable. We can examine each matchbox, count every bead, and trace exactly how experience shapes behavior.\nThis mechanism has a natural Bayesian interpretation that forms the core of my thesis. The bead counts are Dirichlet concentration parameters, and drawing a bead uniformly implements posterior predictive probability matching.\nI will develop this connection in Part II."
  },
  {
    "objectID": "slides/01-menace-machine.html#what-is-menace",
    "href": "slides/01-menace-machine.html#what-is-menace",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Physical Components\n\n287 matchboxes (one per board position requiring a decision)\nColored beads (9 colors for 9 board positions)\nEach bead represents a possible move\n\nLearning Mechanism\n\nDraw a bead → make that move\nWin: add 3 beads of that color\nDraw: add 1 bead\nLoss: remove 1 bead\n\n\n\n\n\nThe +3/+1/−1 values encode preferences over outcomes → map to Active Inference priors.\n\nMENACE comprises 287 matchboxes, each representing a canonical board position where it is the X’s turn to move and a genuine decision must be made.\nEach matchbox contains colored beads — nine colors corresponding to the nine board positions. To select a move, a bead is drawn at random. The color indicates which square to play.\nThe learning mechanism is simple: after each game, the operator revisits every matchbox that was used. If MENACE wins, three beads of the drawn color are added. If the game is a draw, one bead is added. If MENACE loses, one bead of that color is removed.\nThe +3/+1/−1 reinforcement schedule is not arbitrary — these numbers encode preferences over outcomes. They determine how quickly the system converges and how it trades off different outcomes. As we will see, these preferences map directly to prior preferences in Active Inference.\nThis physical transparency is precisely what makes MENACE valuable for theoretical analysis. Unlike neural networks or complex algorithms where the learning dynamics are opaque, every aspect of MENACE’s state is directly observable. We can examine each matchbox, count every bead, and trace exactly how experience shapes behavior.\nThis mechanism has a natural Bayesian interpretation that forms the core of my thesis. The bead counts are Dirichlet concentration parameters, and drawing a bead uniformly implements posterior predictive probability matching.\nI will develop this connection in Part II."
  },
  {
    "objectID": "slides/01-menace-machine.html#reducing-complexity-through-symmetry",
    "href": "slides/01-menace-machine.html#reducing-complexity-through-symmetry",
    "title": "MENACE as a Bayesian Observer",
    "section": "Reducing Complexity Through Symmetry",
    "text": "Reducing Complexity Through Symmetry\n\n\nWithout Symmetry\n\n5,478 distinct legal positions\nImpractical to build\n\nWith Symmetry\n\n8 symmetries (4 rotations × 2 reflections)\n765 canonical positions after symmetry reduction\n338 X-to-move → 304 (prune forced) → 287 (prune double-threats)\nManageable physical system\n\n\n\n\n\n\nSymmetry reduction is essential for making MENACE physically constructible.\nWithout it, there are thousands distinct legal board positions — far too many for a physical system. The Tic-Tac-Toe board has eight symmetries: four rotations and four reflections, forming the D4 dihedral group. Positions that differ only by symmetry are strategically equivalent, so they can share a single matchbox.\nAfter symmetry reduction, these 5,478 positions collapse to just 765 canonical positions. Restricting to X-to-move positions gives 338 states. Pruning forced moves — positions where only one legal move exists — yields 304 states.\nMichie’s original design further removes 17 double-threat positions where the outcome is already determined, yielding the final 287 states — the minimal representation of genuine decision points in the game."
  },
  {
    "objectID": "slides/03-free-energy-principle.html",
    "href": "slides/03-free-energy-principle.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Core idea: Adaptive systems minimize surprise — but surprise is intractable.\n\\[\\text{Surprise} = -\\ln p(o) \\quad \\text{(requires marginalizing over hidden states)}\\]\nSolution: Minimize free energy \\(F\\), a computable upper bound on surprise.\n\\[F = \\underbrace{D_{KL}[q(s|o) \\| p(s|o)]}_{\\geq 0} + \\underbrace{(-\\ln p(o))}_{\\text{surprise}} \\geq \\text{surprise}\\]\nActive Inference operationalizes the FEP: update beliefs to match observations, or act to make observations match beliefs.\n\n\\(F\\): free energy   •   \\(D_{KL}\\): KL divergence   •   \\(q(s|o)\\): approximate posterior   •   \\(p(s|o)\\): true posterior   •   \\(p(o)\\): evidence\n\n\nThe Free Energy Principle proposes that adaptive systems — biological or artificial — can be understood as minimizing surprise about their observations. Surprise is defined as negative log probability: \\(-\\ln p(o)\\).\nBut there is a computational problem: computing surprise directly requires marginalizing over all hidden states, which is intractable. This is why we use free energy instead.\nFree energy equals KL divergence plus surprise. Since KL divergence is non-negative, free energy is always an upper bound on surprise. Minimizing free energy does two things: it makes the approximate posterior closer to the true posterior, and it tightens the bound on surprise.\nActive Inference operationalizes the Free Energy Principle. It provides a unified framework for both perception and action: agents can minimize surprise by updating their beliefs to better predict observations (perception), or by acting to generate observations they expect (action).\nMENACE learns policies — it changes its action selection — to make winning unsurprising. This is the key connection to Active Inference that we will develop."
  },
  {
    "objectID": "slides/03-free-energy-principle.html#the-free-energy-principle-and-active-inference",
    "href": "slides/03-free-energy-principle.html#the-free-energy-principle-and-active-inference",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Core idea: Adaptive systems minimize surprise — but surprise is intractable.\n\\[\\text{Surprise} = -\\ln p(o) \\quad \\text{(requires marginalizing over hidden states)}\\]\nSolution: Minimize free energy \\(F\\), a computable upper bound on surprise.\n\\[F = \\underbrace{D_{KL}[q(s|o) \\| p(s|o)]}_{\\geq 0} + \\underbrace{(-\\ln p(o))}_{\\text{surprise}} \\geq \\text{surprise}\\]\nActive Inference operationalizes the FEP: update beliefs to match observations, or act to make observations match beliefs.\n\n\\(F\\): free energy   •   \\(D_{KL}\\): KL divergence   •   \\(q(s|o)\\): approximate posterior   •   \\(p(s|o)\\): true posterior   •   \\(p(o)\\): evidence\n\n\nThe Free Energy Principle proposes that adaptive systems — biological or artificial — can be understood as minimizing surprise about their observations. Surprise is defined as negative log probability: \\(-\\ln p(o)\\).\nBut there is a computational problem: computing surprise directly requires marginalizing over all hidden states, which is intractable. This is why we use free energy instead.\nFree energy equals KL divergence plus surprise. Since KL divergence is non-negative, free energy is always an upper bound on surprise. Minimizing free energy does two things: it makes the approximate posterior closer to the true posterior, and it tightens the bound on surprise.\nActive Inference operationalizes the Free Energy Principle. It provides a unified framework for both perception and action: agents can minimize surprise by updating their beliefs to better predict observations (perception), or by acting to generate observations they expect (action).\nMENACE learns policies — it changes its action selection — to make winning unsurprising. This is the key connection to Active Inference that we will develop."
  },
  {
    "objectID": "slides/03-free-energy-principle.html#expected-free-energy-with-λ-parameter",
    "href": "slides/03-free-energy-principle.html#expected-free-energy-with-λ-parameter",
    "title": "MENACE as a Bayesian Observer",
    "section": "Expected Free Energy with λ Parameter",
    "text": "Expected Free Energy with λ Parameter\nThe epistemic weight \\(\\lambda\\) prices information in outcome units:\n\\[G_\\lambda(\\pi) = \\underbrace{\\text{Risk}(\\pi)}_{\\text{instrumental cost}} - \\lambda \\underbrace{I(o;\\theta)}_{\\text{epistemic value}}\\]\n\nRisk: \\(D_{KL}(q(o|\\pi) \\| p(o|C))\\) — distance from preferred outcomes\nEpistemic Value: \\(I(o;\\theta)\\) — expected information gain about parameters\n\n\\(\\lambda\\) is Michie’s “exchange rate” between information and immediate gain.\nDecision objective: Expected free energy makes the utility + information trade-off explicit.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\pi\\): policy   •   \\(\\lambda\\): epistemic weight   •   \\(o\\): outcome   •   \\(\\theta\\): model parameters   •   \\(I(o;\\theta)\\): information gain about \\(\\theta\\) from observing \\(o\\)   •   \\(q(o|\\pi)\\): predicted outcomes   •   \\(C\\): prior preferences   •   \\(p(o|C)\\): preferred outcomes\n\n\nVariational free energy handles inference about the present. For planning — choosing actions under uncertainty — Active Inference extends this to expected free energy, which evaluates policies by their anticipated consequences.\nThe expected free energy \\(G\\) parameterized by \\(\\lambda\\) for policy \\(\\pi\\) equals the risk under that policy minus \\(\\lambda\\) times the information gain about parameters \\(\\theta\\) from observing an outcome \\(o\\).\nThe decomposition shown here separates two components:\n\nRisk is the KL divergence between the predicted outcome distribution and the agent’s preferences. It measures how far expected outcomes are from desired outcomes.\nEpistemic value is the mutual information between observations and model parameters. It measures how much the agent expects to learn.\n\nThe \\(\\lambda\\) parameter weights the epistemic term.\nWhen \\(\\lambda\\) equals zero, the agent pursues only instrumental goals — minimizing risk, maximizing preferred outcomes.\nWhen \\(\\lambda\\) is positive, the agent trades off immediate performance against information gain.\nThis is precisely the exchange rate Michie requested. The \\(\\lambda\\) parameter prices “information for future use” in the same units as “present expected gain.” Different values of \\(\\lambda\\) yield different exploration-exploitation trade-offs, and my thesis tests this empirically.\nNote that the epistemic term enters with a negative sign because we minimize \\(G\\) — more information gain reduces expected free energy, making information-seeking policies more attractive under minimization."
  },
  {
    "objectID": "slides/02-bayesian-connection.html",
    "href": "slides/02-bayesian-connection.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Matchbox State\n\\[\\theta_s \\sim \\text{Dir}(\\alpha_s)\\]\nwhere \\(\\alpha_s\\) = bead counts\n\nAction Selection\n\nBead counts → move probabilities (Dirichlet posterior predictive)\nUniform bead draw = sample from \\(\\text{Cat}(\\alpha_s/\\alpha_{s,0})\\)\n\n\n\nPosterior Predictive Probability Matching\nDrawing a bead uniformly = sampling from the posterior predictive\n\\[P(a|s) = E[\\theta_{s,a}] = \\frac{\\alpha_{s,a}}{\\alpha_{s,0}}\\]\nThesis contribution: This mapping formalizes MENACE’s implicit Bayesian structure.\n\n\\(\\theta_s\\): move probability vector for state \\(s\\)   •   \\(\\alpha_s\\): bead counts (Dirichlet parameters)   •   \\(\\alpha_{s,0} = \\sum_a \\alpha_{s,a}\\): total beads   •   \\(\\text{Dir}\\): Dirichlet distribution   •   \\(\\text{Cat}\\): categorical distribution\n\n\nThis slide establishes the core mathematical correspondence.\nEach matchbox can be viewed as maintaining a Dirichlet distribution over move probabilities, where the bead counts are the concentration parameters \\(\\alpha\\).\nWhat does it mean to draw a bead uniformly from the matchbox? In Bayesian terms, this is sampling from the posterior predictive distribution.\nThe posterior predictive answers: “Given everything I’ve learned, what action should I take?” It integrates over all possible parameter values, weighted by how plausible they are.\nFor the Dirichlet-categorical pair, this has a simple form: the probability of choosing action \\(a\\) is just the proportion of beads — \\(\\alpha_a / \\alpha_0\\). This is called probability matching: the agent’s action probabilities directly reflect its learned beliefs.\nThis is distinct from Thompson sampling — the standard Bayesian approach in bandit problems — which would draw a full parameter vector \\(\\theta\\) from the Dirichlet and then take the argmax.\nProbability matching naturally produces mixed strategies: actions are selected proportionally to their weights rather than always choosing the highest-weighted action. This matters in adversarial settings — a deterministic policy is predictable, allowing opponents to learn and counter it. Mixed strategies maintain unpredictability, which is why Nash equilibria in many games involve randomization.\nThe Dirichlet-categorical pairing is significant because it is a conjugate pair: the posterior has the same functional form as the prior, just with updated parameters. This conjugacy is what makes MENACE’s bead arithmetic a valid form of Bayesian updating — at least for positive reinforcement. Loss updates, which remove beads, are a heuristic rather than a literal Bayesian update."
  },
  {
    "objectID": "slides/02-bayesian-connection.html#menace-implements-dirichlet-categorical-inference",
    "href": "slides/02-bayesian-connection.html#menace-implements-dirichlet-categorical-inference",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Matchbox State\n\\[\\theta_s \\sim \\text{Dir}(\\alpha_s)\\]\nwhere \\(\\alpha_s\\) = bead counts\n\nAction Selection\n\nBead counts → move probabilities (Dirichlet posterior predictive)\nUniform bead draw = sample from \\(\\text{Cat}(\\alpha_s/\\alpha_{s,0})\\)\n\n\n\nPosterior Predictive Probability Matching\nDrawing a bead uniformly = sampling from the posterior predictive\n\\[P(a|s) = E[\\theta_{s,a}] = \\frac{\\alpha_{s,a}}{\\alpha_{s,0}}\\]\nThesis contribution: This mapping formalizes MENACE’s implicit Bayesian structure.\n\n\\(\\theta_s\\): move probability vector for state \\(s\\)   •   \\(\\alpha_s\\): bead counts (Dirichlet parameters)   •   \\(\\alpha_{s,0} = \\sum_a \\alpha_{s,a}\\): total beads   •   \\(\\text{Dir}\\): Dirichlet distribution   •   \\(\\text{Cat}\\): categorical distribution\n\n\nThis slide establishes the core mathematical correspondence.\nEach matchbox can be viewed as maintaining a Dirichlet distribution over move probabilities, where the bead counts are the concentration parameters \\(\\alpha\\).\nWhat does it mean to draw a bead uniformly from the matchbox? In Bayesian terms, this is sampling from the posterior predictive distribution.\nThe posterior predictive answers: “Given everything I’ve learned, what action should I take?” It integrates over all possible parameter values, weighted by how plausible they are.\nFor the Dirichlet-categorical pair, this has a simple form: the probability of choosing action \\(a\\) is just the proportion of beads — \\(\\alpha_a / \\alpha_0\\). This is called probability matching: the agent’s action probabilities directly reflect its learned beliefs.\nThis is distinct from Thompson sampling — the standard Bayesian approach in bandit problems — which would draw a full parameter vector \\(\\theta\\) from the Dirichlet and then take the argmax.\nProbability matching naturally produces mixed strategies: actions are selected proportionally to their weights rather than always choosing the highest-weighted action. This matters in adversarial settings — a deterministic policy is predictable, allowing opponents to learn and counter it. Mixed strategies maintain unpredictability, which is why Nash equilibria in many games involve randomization.\nThe Dirichlet-categorical pairing is significant because it is a conjugate pair: the posterior has the same functional form as the prior, just with updated parameters. This conjugacy is what makes MENACE’s bead arithmetic a valid form of Bayesian updating — at least for positive reinforcement. Loss updates, which remove beads, are a heuristic rather than a literal Bayesian update."
  },
  {
    "objectID": "slides/02-bayesian-connection.html#bayesian-updates-through-bead-manipulation",
    "href": "slides/02-bayesian-connection.html#bayesian-updates-through-bead-manipulation",
    "title": "MENACE as a Bayesian Observer",
    "section": "Bayesian Updates Through Bead Manipulation",
    "text": "Bayesian Updates Through Bead Manipulation\nStandard Bayesian update \\[\\text{Prior} + \\text{Data} = \\text{Posterior}\\]\nIn Dirichlet terms \\[\\text{Dir}(\\alpha) + \\text{observation} = \\text{Dir}(\\alpha + 1)\\]\nMENACE’s version \\[\\alpha_{s,a} \\leftarrow \\alpha_{s,a} + U(o)\\]\nwhere \\(U(o) \\in \\{3, 1, -1\\}\\) is a utility-weighted pseudo-count update.\nLoss updates are a heuristic penalty/forgetting step, not a literal conjugate posterior.\n\nThis slide makes precise the sense in which MENACE’s updates are “quasi-Bayesian.” Standard Dirichlet-categorical conjugacy says that observing an outcome increments the corresponding count by one. MENACE’s updates differ in two ways.\nFirst, the increments are weighted by utility: +3 for wins, +1 for draws. This is not standard Bayesian updating, but it can be interpreted as utility-weighted pseudo-count reinforcement — the agent effectively treats a win as stronger evidence than a draw.\nSecond, and more importantly, loss updates remove beads. Strict Bayesian updating only adds evidence. There is no conjugate operation for “unlearning.” The bead removal is a heuristic penalty that shifts probability away from moves that led to losses. Restocking — adding beads back when a matchbox empties — prevents the system from becoming stuck.\nMy thesis characterizes this as “directionally aligned” with reducing instrumental risk: the updates push the policy toward preferred outcomes, even if they are not exact gradient steps of a variational objective. This qualification is important for the theoretical claims."
  },
  {
    "objectID": "slides/06-answering-michies-question.html",
    "href": "slides/06-answering-michies-question.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "“The difficulty lies in costing the acquisition of information for future use at the expense of present expected gain.”\n\n\\[G_\\lambda(\\pi) = \\text{Risk}(\\pi) - \\lambda \\cdot I(o;\\theta)\\]\nThe scalar \\(\\lambda \\geq 0\\) is the exchange rate Michie asked for — not a single algorithm, but a family:\n\n\nActive Inference (General)\n\nEpistemic value priced via \\(\\lambda\\)\nTrade-off is a design parameter\n\n\nMENACE (Special Case)\n\nInstrumental objective (\\(\\lambda = 0\\))\nExploration emerges from uncertainty\n\n\n\nThesis contribution: This framework answers Michie’s question with MENACE as a concrete, mechanizable special case.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\lambda\\): exchange rate   •   \\(I(o;\\theta)\\): information gain   •   \\(\\text{Risk}\\): instrumental cost\n\n\nFinally, we return to Michie’s original question: how to price information against immediate performance?\nThe \\(\\lambda\\) parameter is the exchange rate. When \\(\\lambda = 0\\), information has no explicit value — the agent is purely instrumental. When \\(\\lambda\\) is positive, the agent sacrifices immediate performance to reduce uncertainty.\nThe answer is not a single optimal algorithm but a family of objectives. MENACE implements \\(\\lambda = 0\\) with implicit exploration from probability matching. Active Inference provides the general framework where the trade-off becomes an explicit design choice.\nMy thesis places MENACE within this modern framework as a mechanically interpretable special case."
  },
  {
    "objectID": "slides/06-answering-michies-question.html#answering-michies-question",
    "href": "slides/06-answering-michies-question.html#answering-michies-question",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "“The difficulty lies in costing the acquisition of information for future use at the expense of present expected gain.”\n\n\\[G_\\lambda(\\pi) = \\text{Risk}(\\pi) - \\lambda \\cdot I(o;\\theta)\\]\nThe scalar \\(\\lambda \\geq 0\\) is the exchange rate Michie asked for — not a single algorithm, but a family:\n\n\nActive Inference (General)\n\nEpistemic value priced via \\(\\lambda\\)\nTrade-off is a design parameter\n\n\nMENACE (Special Case)\n\nInstrumental objective (\\(\\lambda = 0\\))\nExploration emerges from uncertainty\n\n\n\nThesis contribution: This framework answers Michie’s question with MENACE as a concrete, mechanizable special case.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\lambda\\): exchange rate   •   \\(I(o;\\theta)\\): information gain   •   \\(\\text{Risk}\\): instrumental cost\n\n\nFinally, we return to Michie’s original question: how to price information against immediate performance?\nThe \\(\\lambda\\) parameter is the exchange rate. When \\(\\lambda = 0\\), information has no explicit value — the agent is purely instrumental. When \\(\\lambda\\) is positive, the agent sacrifices immediate performance to reduce uncertainty.\nThe answer is not a single optimal algorithm but a family of objectives. MENACE implements \\(\\lambda = 0\\) with implicit exploration from probability matching. Active Inference provides the general framework where the trade-off becomes an explicit design choice.\nMy thesis places MENACE within this modern framework as a mechanically interpretable special case."
  },
  {
    "objectID": "presentation.html#michies-question-1966",
    "href": "presentation.html#michies-question-1966",
    "title": "MENACE as a Bayesian Observer",
    "section": "Michie’s Question (1966)",
    "text": "Michie’s Question (1966)\n\n\n\n“In simple games for which individual storage of all past board positions is feasible, is any optimal learning algorithm known? … The difficulty lies in costing the acquisition of information for future use at the expense of present expected gain.”\n— Donald Michie, “Game-Playing and Game-Learning Automata” (1966)\n\n\n\n\n\nDonald Michie\n\n\n\nExpected free energy provides a Bayes-optimal formalization of the trade-off Michie identified.\n\nDonald Michie was a British AI pioneer who worked alongside Alan Turing at Bletchley Park.\nIn 1961, he built MENACE — a physical machine that learned to play Tic-Tac-Toe using matchboxes and colored beads.\nBy 1966, he was asking a deeper question: given that we can store all board positions, what is the optimal way to learn?\nThe difficulty, as he put it, lies in pricing information — how much immediate performance should an agent sacrifice to gather knowledge that might be useful later? This is the exploration-exploitation trade-off stated in its original form.\nMy thesis argues that expected free energy — an objective function from Bayesian inference — provides the formal answer Michie sought.\nI will show how MENACE implements a special case of this objective and present empirical evidence supporting this correspondence."
  },
  {
    "objectID": "presentation.html#thesis-contributions",
    "href": "presentation.html#thesis-contributions",
    "title": "MENACE as a Bayesian Observer",
    "section": "Thesis Contributions",
    "text": "Thesis Contributions\n\nA mapping from MENACE to Active Inference in Dirichlet-categorical terms\nIdentification of MENACE as an instrumental Active Inference agent\nEmpirical comparison with Active Inference variants and Reinforcement Learning (RL) baselines\nA generative model in which \\(\\lambda\\) parameterizes the cost of information acquisition\n\n\nMy thesis makes four contributions.\nFirst, a precise mapping from MENACE’s mechanics to Active Inference using Dirichlet-categorical distributions.\nSecond, the identification that MENACE approximately corresponds to the purely instrumental special case of expected free energy — an agent that exploits without explicit information-seeking.\nThird, empirical validation comparing MENACE against Active Inference variants and Reinforcement Learning baselines.\nFourth, a generative model within the Active Inference framework that explicitly parameterizes the cost of information acquisition which directly answers Michie’s question by providing a formal mechanism to price information in the same units as outcomes.\nI will now present the background on MENACE before developing this correspondence."
  },
  {
    "objectID": "presentation.html#what-is-menace",
    "href": "presentation.html#what-is-menace",
    "title": "MENACE as a Bayesian Observer",
    "section": "What is MENACE?",
    "text": "What is MENACE?\n\n\nPhysical Components\n\n287 matchboxes (one per board position requiring a decision)\nColored beads (9 colors for 9 board positions)\nEach bead represents a possible move\n\nLearning Mechanism\n\nDraw a bead → make that move\nWin: add 3 beads of that color\nDraw: add 1 bead\nLoss: remove 1 bead\n\n\n\n\nThe +3/+1/−1 values encode preferences over outcomes → map to Active Inference priors.\n\nMENACE comprises 287 matchboxes, each representing a canonical board position where it is the X’s turn to move and a genuine decision must be made.\nEach matchbox contains colored beads — nine colors corresponding to the nine board positions. To select a move, a bead is drawn at random. The color indicates which square to play.\nThe learning mechanism is simple: after each game, the operator revisits every matchbox that was used. If MENACE wins, three beads of the drawn color are added. If the game is a draw, one bead is added. If MENACE loses, one bead of that color is removed.\nThe +3/+1/−1 reinforcement schedule is not arbitrary — these numbers encode preferences over outcomes. They determine how quickly the system converges and how it trades off different outcomes. As we will see, these preferences map directly to prior preferences in Active Inference.\nThis physical transparency is precisely what makes MENACE valuable for theoretical analysis. Unlike neural networks or complex algorithms where the learning dynamics are opaque, every aspect of MENACE’s state is directly observable. We can examine each matchbox, count every bead, and trace exactly how experience shapes behavior.\nThis mechanism has a natural Bayesian interpretation that forms the core of my thesis. The bead counts are Dirichlet concentration parameters, and drawing a bead uniformly implements posterior predictive probability matching.\nI will develop this connection in Part II."
  },
  {
    "objectID": "presentation.html#reducing-complexity-through-symmetry",
    "href": "presentation.html#reducing-complexity-through-symmetry",
    "title": "MENACE as a Bayesian Observer",
    "section": "Reducing Complexity Through Symmetry",
    "text": "Reducing Complexity Through Symmetry\n\n\nWithout Symmetry\n\n5,478 distinct legal positions\nImpractical to build\n\nWith Symmetry\n\n8 symmetries (4 rotations × 2 reflections)\n765 canonical positions after symmetry reduction\n338 X-to-move → 304 (prune forced) → 287 (prune double-threats)\nManageable physical system\n\n\n\n\n\nSymmetry reduction is essential for making MENACE physically constructible.\nWithout it, there are thousands distinct legal board positions — far too many for a physical system. The Tic-Tac-Toe board has eight symmetries: four rotations and four reflections, forming the D4 dihedral group. Positions that differ only by symmetry are strategically equivalent, so they can share a single matchbox.\nAfter symmetry reduction, these 5,478 positions collapse to just 765 canonical positions. Restricting to X-to-move positions gives 338 states. Pruning forced moves — positions where only one legal move exists — yields 304 states.\nMichie’s original design further removes 17 double-threat positions where the outcome is already determined, yielding the final 287 states — the minimal representation of genuine decision points in the game."
  },
  {
    "objectID": "presentation.html#menace-implements-dirichlet-categorical-inference",
    "href": "presentation.html#menace-implements-dirichlet-categorical-inference",
    "title": "MENACE as a Bayesian Observer",
    "section": "MENACE Implements Dirichlet-Categorical Inference",
    "text": "MENACE Implements Dirichlet-Categorical Inference\n\n\nMatchbox State\n\\[\\theta_s \\sim \\text{Dir}(\\alpha_s)\\]\nwhere \\(\\alpha_s\\) = bead counts\n\nAction Selection\n\nBead counts → move probabilities (Dirichlet posterior predictive)\nUniform bead draw = sample from \\(\\text{Cat}(\\alpha_s/\\alpha_{s,0})\\)\n\n\nPosterior Predictive Probability Matching\nDrawing a bead uniformly = sampling from the posterior predictive\n\\[P(a|s) = E[\\theta_{s,a}] = \\frac{\\alpha_{s,a}}{\\alpha_{s,0}}\\]\nThesis contribution: This mapping formalizes MENACE’s implicit Bayesian structure.\n\n\\(\\theta_s\\): move probability vector for state \\(s\\)   •   \\(\\alpha_s\\): bead counts (Dirichlet parameters)   •   \\(\\alpha_{s,0} = \\sum_a \\alpha_{s,a}\\): total beads   •   \\(\\text{Dir}\\): Dirichlet distribution   •   \\(\\text{Cat}\\): categorical distribution\n\n\nThis slide establishes the core mathematical correspondence.\nEach matchbox can be viewed as maintaining a Dirichlet distribution over move probabilities, where the bead counts are the concentration parameters \\(\\alpha\\).\nWhat does it mean to draw a bead uniformly from the matchbox? In Bayesian terms, this is sampling from the posterior predictive distribution.\nThe posterior predictive answers: “Given everything I’ve learned, what action should I take?” It integrates over all possible parameter values, weighted by how plausible they are.\nFor the Dirichlet-categorical pair, this has a simple form: the probability of choosing action \\(a\\) is just the proportion of beads — \\(\\alpha_a / \\alpha_0\\). This is called probability matching: the agent’s action probabilities directly reflect its learned beliefs.\nThis is distinct from Thompson sampling — the standard Bayesian approach in bandit problems — which would draw a full parameter vector \\(\\theta\\) from the Dirichlet and then take the argmax.\nProbability matching naturally produces mixed strategies: actions are selected proportionally to their weights rather than always choosing the highest-weighted action. This matters in adversarial settings — a deterministic policy is predictable, allowing opponents to learn and counter it. Mixed strategies maintain unpredictability, which is why Nash equilibria in many games involve randomization.\nThe Dirichlet-categorical pairing is significant because it is a conjugate pair: the posterior has the same functional form as the prior, just with updated parameters. This conjugacy is what makes MENACE’s bead arithmetic a valid form of Bayesian updating — at least for positive reinforcement. Loss updates, which remove beads, are a heuristic rather than a literal Bayesian update."
  },
  {
    "objectID": "presentation.html#bayesian-updates-through-bead-manipulation",
    "href": "presentation.html#bayesian-updates-through-bead-manipulation",
    "title": "MENACE as a Bayesian Observer",
    "section": "Bayesian Updates Through Bead Manipulation",
    "text": "Bayesian Updates Through Bead Manipulation\nStandard Bayesian update \\[\\text{Prior} + \\text{Data} = \\text{Posterior}\\]\nIn Dirichlet terms \\[\\text{Dir}(\\alpha) + \\text{observation} = \\text{Dir}(\\alpha + 1)\\]\nMENACE’s version \\[\\alpha_{s,a} \\leftarrow \\alpha_{s,a} + U(o)\\]\nwhere \\(U(o) \\in \\{3, 1, -1\\}\\) is a utility-weighted pseudo-count update.\nLoss updates are a heuristic penalty/forgetting step, not a literal conjugate posterior.\n\nThis slide makes precise the sense in which MENACE’s updates are “quasi-Bayesian.” Standard Dirichlet-categorical conjugacy says that observing an outcome increments the corresponding count by one. MENACE’s updates differ in two ways.\nFirst, the increments are weighted by utility: +3 for wins, +1 for draws. This is not standard Bayesian updating, but it can be interpreted as utility-weighted pseudo-count reinforcement — the agent effectively treats a win as stronger evidence than a draw.\nSecond, and more importantly, loss updates remove beads. Strict Bayesian updating only adds evidence. There is no conjugate operation for “unlearning.” The bead removal is a heuristic penalty that shifts probability away from moves that led to losses. Restocking — adding beads back when a matchbox empties — prevents the system from becoming stuck.\nMy thesis characterizes this as “directionally aligned” with reducing instrumental risk: the updates push the policy toward preferred outcomes, even if they are not exact gradient steps of a variational objective. This qualification is important for the theoretical claims."
  },
  {
    "objectID": "presentation.html#the-free-energy-principle-and-active-inference",
    "href": "presentation.html#the-free-energy-principle-and-active-inference",
    "title": "MENACE as a Bayesian Observer",
    "section": "The Free Energy Principle and Active Inference",
    "text": "The Free Energy Principle and Active Inference\nCore idea: Adaptive systems minimize surprise — but surprise is intractable.\n\\[\\text{Surprise} = -\\ln p(o) \\quad \\text{(requires marginalizing over hidden states)}\\]\nSolution: Minimize free energy \\(F\\), a computable upper bound on surprise.\n\\[F = \\underbrace{D_{KL}[q(s|o) \\| p(s|o)]}_{\\geq 0} + \\underbrace{(-\\ln p(o))}_{\\text{surprise}} \\geq \\text{surprise}\\]\nActive Inference operationalizes the FEP: update beliefs to match observations, or act to make observations match beliefs.\n\n\\(F\\): free energy   •   \\(D_{KL}\\): KL divergence   •   \\(q(s|o)\\): approximate posterior   •   \\(p(s|o)\\): true posterior   •   \\(p(o)\\): evidence\n\n\nThe Free Energy Principle proposes that adaptive systems — biological or artificial — can be understood as minimizing surprise about their observations. Surprise is defined as negative log probability: \\(-\\ln p(o)\\).\nBut there is a computational problem: computing surprise directly requires marginalizing over all hidden states, which is intractable. This is why we use free energy instead.\nFree energy equals KL divergence plus surprise. Since KL divergence is non-negative, free energy is always an upper bound on surprise. Minimizing free energy does two things: it makes the approximate posterior closer to the true posterior, and it tightens the bound on surprise.\nActive Inference operationalizes the Free Energy Principle. It provides a unified framework for both perception and action: agents can minimize surprise by updating their beliefs to better predict observations (perception), or by acting to generate observations they expect (action).\nMENACE learns policies — it changes its action selection — to make winning unsurprising. This is the key connection to Active Inference that we will develop."
  },
  {
    "objectID": "presentation.html#expected-free-energy-with-λ-parameter",
    "href": "presentation.html#expected-free-energy-with-λ-parameter",
    "title": "MENACE as a Bayesian Observer",
    "section": "Expected Free Energy with λ Parameter",
    "text": "Expected Free Energy with λ Parameter\nThe epistemic weight \\(\\lambda\\) prices information in outcome units:\n\\[G_\\lambda(\\pi) = \\underbrace{\\text{Risk}(\\pi)}_{\\text{instrumental cost}} - \\lambda \\underbrace{I(o;\\theta)}_{\\text{epistemic value}}\\]\n\nRisk: \\(D_{KL}(q(o|\\pi) \\| p(o|C))\\) — distance from preferred outcomes\nEpistemic Value: \\(I(o;\\theta)\\) — expected information gain about parameters\n\n\\(\\lambda\\) is Michie’s “exchange rate” between information and immediate gain.\nDecision objective: Expected free energy makes the utility + information trade-off explicit.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\pi\\): policy   •   \\(\\lambda\\): epistemic weight   •   \\(o\\): outcome   •   \\(\\theta\\): model parameters   •   \\(I(o;\\theta)\\): information gain about \\(\\theta\\) from observing \\(o\\)   •   \\(q(o|\\pi)\\): predicted outcomes   •   \\(C\\): prior preferences   •   \\(p(o|C)\\): preferred outcomes\n\n\nVariational free energy handles inference about the present. For planning — choosing actions under uncertainty — Active Inference extends this to expected free energy, which evaluates policies by their anticipated consequences.\nThe expected free energy \\(G\\) parameterized by \\(\\lambda\\) for policy \\(\\pi\\) equals the risk under that policy minus \\(\\lambda\\) times the information gain about parameters \\(\\theta\\) from observing an outcome \\(o\\).\nThe decomposition shown here separates two components:\n\nRisk is the KL divergence between the predicted outcome distribution and the agent’s preferences. It measures how far expected outcomes are from desired outcomes.\nEpistemic value is the mutual information between observations and model parameters. It measures how much the agent expects to learn.\n\nThe \\(\\lambda\\) parameter weights the epistemic term.\nWhen \\(\\lambda\\) equals zero, the agent pursues only instrumental goals — minimizing risk, maximizing preferred outcomes.\nWhen \\(\\lambda\\) is positive, the agent trades off immediate performance against information gain.\nThis is precisely the exchange rate Michie requested. The \\(\\lambda\\) parameter prices “information for future use” in the same units as “present expected gain.” Different values of \\(\\lambda\\) yield different exploration-exploitation trade-offs, and my thesis tests this empirically.\nNote that the epistemic term enters with a negative sign because we minimize \\(G\\) — more information gain reduces expected free energy, making information-seeking policies more attractive under minimization."
  },
  {
    "objectID": "presentation.html#menaces-implicit-generative-model",
    "href": "presentation.html#menaces-implicit-generative-model",
    "title": "MENACE as a Bayesian Observer",
    "section": "MENACE’s Implicit Generative Model",
    "text": "MENACE’s Implicit Generative Model\nWhat MENACE “believes”\n\\[p(o, s_{0:T}, a_{0:T}) = p(o|s_T) \\prod_{t} p(s_{t+1}|s_t, a_t) p(a_t|s_t)\\]\nwhere:\n\n\\(p(o|s_T)\\): Outcome model — final position determines outcome (deterministic)\n\\(p(s_{t+1}|s_t, a_t)\\): Transition model — game rules (fixed)\n\\(p(a_t|s_t)\\): Policy to learn (the beads)\n\nAll of MENACE’s learning focuses on optimizing the policy component.\n\n\\(o\\): outcome   •   \\(s_t\\): state at time \\(t\\)   •   \\(a_t\\): action at time \\(t\\)   •   \\(T\\): terminal time   •   \\(p(o|s_T)\\): outcome model   •   \\(p(s_{t+1}|s_t,a_t)\\): transition model   •   \\(p(a_t|s_t)\\): policy\n\n\nTo apply Active Inference to MENACE, we must specify its implicit generative model — the probabilistic model of how games unfold. This factorization has three components.\nFirst, the outcome model \\(p(o|s_T)\\): terminal board positions deterministically map to outcomes. MENACE encodes this implicitly — reinforcement is applied based on game results, so the system “knows” which positions are winning, losing, or drawing.\nSecond, the transition model \\(p(s_{t+1}|s_t, a_t)\\): the rules of Tic-Tac-Toe. These are fixed and encoded in how the human operator uses the system — the operator opens the matchbox corresponding to the current board state, so transitions are handled externally.\nThird, the policy \\(p(a_t|s_t)\\): this is what the beads represent. Each matchbox encodes MENACE’s beliefs about which moves are preferable in that position.\nAll of MENACE’s learning focuses on the policy component. The game rules and outcome mappings are fixed. Only the action preferences change.\nThis factorization is the same structure used by modern reinforcement learning algorithms, but implemented in physical hardware."
  },
  {
    "objectID": "presentation.html#preference-weighted-policy-shaping",
    "href": "presentation.html#preference-weighted-policy-shaping",
    "title": "MENACE as a Bayesian Observer",
    "section": "Preference-Weighted Policy Shaping",
    "text": "Preference-Weighted Policy Shaping\nMENACE corresponds to the instrumental objective\n\\[G_\\lambda(\\pi) = \\mathrm{Risk}(\\pi) - \\lambda I(o;\\theta), \\quad \\lambda = 0\\]\n\nPreferences are encoded by \\(U(o) \\in \\{+3,+1,-1\\}\\)\nBead updates shift Dirichlet counts toward preferred outcomes\nNegative updates are heuristic penalties; restocking keeps \\(\\alpha_{s,a} &gt; 0\\)\n\nThe update is directionally aligned with reducing instrumental risk, not an exact gradient step.\nThesis contribution: This model is instantiated for MENACE and Tic-Tac-Toe, with explicit computation of all quantities.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\pi\\): policy   •   \\(o\\): outcome   •   \\(\\theta\\): model parameters (bead proportions)   •   \\(I(o;\\theta)\\): information gain about \\(\\theta\\) from \\(o\\)   •   \\(U(o)\\): utility   •   \\(\\alpha_{s,a}\\): bead count\n\n\nReading the equation: “The expected free energy G sub lambda for policy pi equals the risk under that policy minus lambda times information gain, where lambda equals zero — so only the risk term remains.”\nThis slide states the central theoretical claim. MENACE corresponds to the instrumental special case of expected free energy minimization — that is, \\(\\lambda\\) equals zero, with no explicit epistemic drive.\nThe preferences encoded by the reinforcement schedule (+3/+1/-1) map to the prior preference distribution \\(p(o|C)\\) in the Active Inference formulation. Bead updates shift Dirichlet parameters toward outcomes with higher utility, which is directionally aligned with reducing the risk term — the KL divergence between predicted and preferred outcomes.\nThree important caveats qualify this claim.\nFirst, the update is directionally aligned with reducing risk, not an exact gradient step of a variational objective.\nSecond, loss updates (removing beads) are heuristic rather than conjugate Bayesian updates.\nThird, restocking — adding beads back when matchboxes empty — is a practical mechanism without a clean theoretical interpretation.\nDespite these caveats, my thesis contribution is to make this correspondence concrete: I instantiate this Active Inference model for MENACE and Tic-Tac-Toe, computing all quantities explicitly. This allows empirical testing of whether the theoretical correspondence holds in practice."
  },
  {
    "objectID": "presentation.html#experimental-setup",
    "href": "presentation.html#experimental-setup",
    "title": "MENACE as a Bayesian Observer",
    "section": "Experimental Setup",
    "text": "Experimental Setup\n\n\nAgents Compared\n\nMENACE (Michie filter, box restock)\nInstrumental AIF (\\(\\lambda = 0\\))\nHybrid AIF (\\(\\lambda = 0.5\\))\nPure AIF (\\(\\lambda \\in \\{0, 0.25, 0.5\\}\\))\nQ-Learning / SARSA baselines\n\n\nProtocol\n\nTraining: 500 games (5,000 for RL)\nValidation: 100 games vs. optimal\nSeeds: 10 independent runs\nState filters: Michie (287 states)\nMENACE curriculum: mixed; AIF baseline: optimal-only\n\n\nThesis contribution: First systematic empirical comparison of MENACE against Active Inference variants.\n\nThe theoretical correspondence makes testable predictions. If MENACE corresponds to instrumental Active Inference with \\(\\lambda = 0\\), then an explicit Active Inference agent with \\(\\lambda = 0\\) should achieve similar performance.\nThe experiments compare five agent types:\n\nMENACE with Michie’s original 287-state filter and box-level restocking\nInstrumental Active Inference with \\(\\lambda = 0\\)\nHybrid Active Inference, which combines MENACE-style Dirichlet updates with EFE-based action selection\nPure Active Inference, which uses EFE for both learning and action selection without the Dirichlet machinery\nTabular Reinforcement Learning baselines: Q-learning and SARSA\n\nBoth Hybrid and Pure variants are tested with positive \\(\\lambda\\) values to add explicit epistemic drive. Tabular RL baselines serve as reference points from the reinforcement learning literature.\nSARSA (State-Action-Reward-State-Action) is on-policy temporal difference control: it updates Q-values using the action actually taken next, whereas Q-learning is off-policy and updates using the maximum over next actions.\nAll agents are trained for 500 games, except RL baselines which receive 5,000 games for asymptotic comparison. Post-training validation uses 100 games against optimal play. Results aggregate over 10 independent seeds.\nOne methodological note: MENACE uses a mixed curriculum — starting against random opponents and progressing to optimal — while the AIF baseline trains directly against optimal. This means training trajectories are not directly comparable, but post-training validation against the same opponent provides the fair comparison point."
  },
  {
    "objectID": "presentation.html#key-finding-1-instrumental-equivalence",
    "href": "presentation.html#key-finding-1-instrumental-equivalence",
    "title": "MENACE as a Bayesian Observer",
    "section": "Key Finding 1: Instrumental Equivalence",
    "text": "Key Finding 1: Instrumental Equivalence\n\n\n\nAlgorithm\nDraw Rate (%)\nLoss Rate (%)\n\n\n\n\nMENACE (box restock)\n\\(84.5 \\pm 8.1\\)\n\\(15.5 \\pm 8.1\\)\n\n\nInstrumental AIF (\\(\\lambda=0\\))\n\\(88.1 \\pm 3.9\\)\n\\(11.9 \\pm 3.9\\)\n\n\n\n\nWhen \\(\\lambda = 0\\), the EFE-based policy reduces to a purely instrumental objective, closely matching MENACE’s pseudo-count reinforcement\nHowever, MENACE has implicit exploration: low concentration → high variance → naturally exploratory early behavior\n\nResult: MENACE ≈ Instrumental Active Inference.\n\nThe first key finding tests the central theoretical claim: does MENACE correspond to instrumental Active Inference?\nMENACE achieves 84.5% draw rate; Instrumental AIF with \\(\\lambda = 0\\) achieves 88.1%. Overlapping confidence intervals support the correspondence — both optimize approximately the same objective.\nResidual differences come from implementation details. Note that MENACE has implicit exploration via the Dirichlet: low concentration means high variance, which diminishes as beliefs consolidate."
  },
  {
    "objectID": "presentation.html#key-finding-2-the-value-of-information",
    "href": "presentation.html#key-finding-2-the-value-of-information",
    "title": "MENACE as a Bayesian Observer",
    "section": "Key Finding 2: The Value of Information",
    "text": "Key Finding 2: The Value of Information\n\n\n\nAlgorithm\nDraw Rate (%)\n\n\n\n\nPure AIF (\\(\\lambda = 0.0\\))\n\\(79.7 \\pm 6.5\\)\n\n\nPure AIF (\\(\\lambda = 0.25\\))\n\\(79.1 \\pm 4.8\\)\n\n\nPure AIF (\\(\\lambda = 0.5\\))\n\\(77.0 \\pm 3.7\\)\n\n\n\nParadox: Epistemic variants (\\(\\lambda &gt; 0\\)) do NOT outperform instrumental baseline (\\(\\lambda=0\\)) within 500 games.\nResolution: This reflects Michie’s trade-off in practice: “acquisition of information for future use at the expense of present expected gain”.\n\nThe second finding addresses Michie’s original question: what is the cost of information?\nHigher \\(\\lambda\\) means lower short-term performance: 79.7% at \\(\\lambda = 0\\) down to 77.0% at \\(\\lambda = 0.5\\). This confirms Michie’s prediction — information has value for future decisions, but acquiring it costs immediate performance.\nThis quantifies the trade-off: explicit epistemic value has a measurable short-horizon cost."
  },
  {
    "objectID": "presentation.html#key-finding-3-robustness-vs.-specialization",
    "href": "presentation.html#key-finding-3-robustness-vs.-specialization",
    "title": "MENACE as a Bayesian Observer",
    "section": "Key Finding 3: Robustness vs. Specialization",
    "text": "Key Finding 3: Robustness vs. Specialization\n\n\n\nAlgorithm\nTraining\nDraw Rate (%)\n\n\n\n\nQ-learning\nrandom\n\\(98.0 \\pm \\phantom{0}1.2\\)\n\n\nQ-learning\ndefensive\n\\(10.2 \\pm 30.9\\)\n\n\nSARSA\nrandom\n\\(97.9 \\pm \\phantom{0}1.9\\)\n\n\nSARSA\ndefensive\n\\(20.5 \\pm 40.3\\)\n\n\n\n\nBox-level restocking preserves full support\nProvides implicit “insurance” against distributional shift\nQ-learning can drive Q-values to extremes, zeroing out actions\nMENACE achieves competitive performance with 10× fewer games\n\nDirichlet advantage: MENACE keeps all action probabilities strictly positive.\n\nThe third finding compares MENACE’s robustness against tabular RL baselines.\nRL achieves 98% when training and evaluation match, but collapses under distribution shift — Q-learning trained on defensive drops to 10% against optimal. MENACE’s restocking ensures all actions keep positive probability, providing implicit regularization.\nAlso note sample efficiency: MENACE achieves competitive performance with 500 games versus 5,000 for RL."
  },
  {
    "objectID": "presentation.html#answering-michies-question",
    "href": "presentation.html#answering-michies-question",
    "title": "MENACE as a Bayesian Observer",
    "section": "Answering Michie’s Question",
    "text": "Answering Michie’s Question\n\n“The difficulty lies in costing the acquisition of information for future use at the expense of present expected gain.”\n\n\\[G_\\lambda(\\pi) = \\text{Risk}(\\pi) - \\lambda \\cdot I(o;\\theta)\\]\nThe scalar \\(\\lambda \\geq 0\\) is the exchange rate Michie asked for — not a single algorithm, but a family:\n\n\nActive Inference (General)\n\nEpistemic value priced via \\(\\lambda\\)\nTrade-off is a design parameter\n\n\nMENACE (Special Case)\n\nInstrumental objective (\\(\\lambda = 0\\))\nExploration emerges from uncertainty\n\n\nThesis contribution: This framework answers Michie’s question with MENACE as a concrete, mechanizable special case.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\lambda\\): exchange rate   •   \\(I(o;\\theta)\\): information gain   •   \\(\\text{Risk}\\): instrumental cost\n\n\nFinally, we return to Michie’s original question: how to price information against immediate performance?\nThe \\(\\lambda\\) parameter is the exchange rate. When \\(\\lambda = 0\\), information has no explicit value — the agent is purely instrumental. When \\(\\lambda\\) is positive, the agent sacrifices immediate performance to reduce uncertainty.\nThe answer is not a single optimal algorithm but a family of objectives. MENACE implements \\(\\lambda = 0\\) with implicit exploration from probability matching. Active Inference provides the general framework where the trade-off becomes an explicit design choice.\nMy thesis places MENACE within this modern framework as a mechanically interpretable special case."
  },
  {
    "objectID": "presentation.html#conclusions",
    "href": "presentation.html#conclusions",
    "title": "MENACE as a Bayesian Observer",
    "section": "Conclusions",
    "text": "Conclusions\nKey Findings\n\nDirichlet-categorical mapping formalizes MENACE’s Bayesian structure\nMENACE ≈ instrumental Active Inference (\\(\\lambda = 0\\)) confirmed empirically\n\\(\\lambda\\) quantifies information cost with measurable short-horizon effects\nDirichlet representation provides robustness that tabular RL lacks\n\nMENACE is a historically remarkable, mechanizable special case of Active Inference.\n\nTo summarize the key findings.\nFirst, the theoretical contribution: the Dirichlet-categorical mapping formalizes MENACE’s implicit Bayesian structure.\nSecond, instrumental equivalence: MENACE and an Active Inference agent with epistemic drive suppressed achieve comparable performance, supporting the identification of MENACE as the purely instrumental special case.\nThird, the cost of information: explicit epistemic value (positive \\(\\lambda\\)) reduces short-horizon performance, exactly as Michie predicted. The \\(\\lambda\\) parameter makes this trade-off an explicit design choice.\nFourth, robustness: the Dirichlet representation provides natural protection against distributional shift that tabular RL lacks, because restocking keeps all actions at positive probability.\nThe takeaway is that MENACE — a sixty-year-old physical learning machine — instantiates principles that remain central to modern Bayesian approaches to learning and decision-making."
  },
  {
    "objectID": "presentation.html#a1-donald-michie-1923-2007",
    "href": "presentation.html#a1-donald-michie-1923-2007",
    "title": "MENACE as a Bayesian Observer",
    "section": "A1: Donald Michie (1923-2007)",
    "text": "A1: Donald Michie (1923-2007)\n\n\n\n\nThe Pioneer Behind MENACE\n\nCodebreaker at Bletchley Park during WWII\nWorked alongside Alan Turing on Colossus\nMA, DPhil, DSc in biological sciences (Oxford)\nProfessor of Machine Intelligence, Edinburgh\n\nKey insight: “Programming human intelligence into machines” — inspired by wartime cryptanalysis.\n\nContributions to AI\n\nFounded Edinburgh’s Machine Intelligence unit\nEditor-in-Chief, “Machine Intelligence” series\nDeveloped machine learning into “industrial-strength tool”\n1996 Feigenbaum Medal for ML applications\n\nMENACE (1961): Physical demonstration that learning could be mechanized.\n\n\nDonald Michie was a polymath who bridged biology, computing, and artificial intelligence. His wartime experience at Bletchley Park, working alongside Alan Turing on Colossus, convinced him that machines could exhibit intelligent behavior through systematic processes.\nMENACE — the Machine Educable Noughts And Crosses Engine — was Michie’s 1961 demonstration that learning could emerge from simple mechanical operations: matchboxes, colored beads, and a reinforcement rule."
  },
  {
    "objectID": "presentation.html#a2-dirichlet-categorical-conjugacy-proof",
    "href": "presentation.html#a2-dirichlet-categorical-conjugacy-proof",
    "title": "MENACE as a Bayesian Observer",
    "section": "A2: Dirichlet-Categorical Conjugacy Proof",
    "text": "A2: Dirichlet-Categorical Conjugacy Proof\nTheorem: If \\(\\theta \\sim \\text{Dir}(\\alpha)\\) and \\(n\\) outcomes are observed, then: \\[\\theta|\\text{data} \\sim \\text{Dir}(\\alpha + n)\\]\nProof:\n\\[\n\\begin{align}\np(\\theta|\\text{data}) &\\propto p(\\text{data}|\\theta)p(\\theta) \\\\\n&= \\prod_{i=1}^{k} \\theta_i^{n_i} \\cdot \\frac{1}{B(\\alpha)} \\prod_{i=1}^{k} \\theta_i^{\\alpha_i-1} \\\\\n&\\propto \\prod_{i=1}^{k} \\theta_i^{\\alpha_i + n_i - 1}\n\\end{align}\n\\]\nThis is the kernel of \\(\\text{Dir}(\\alpha + n)\\) ∎\n\nIf \\(\\theta\\) is drawn from a Dirichlet with parameters \\(\\alpha\\), and \\(n\\) outcomes are observed, then \\(\\theta\\) given data is Dirichlet with parameters \\(\\alpha\\) plus \\(n\\).\nThis proof shows why MENACE’s bead-updating mechanism is mathematically elegant.\nThe Dirichlet-categorical conjugacy means that starting with a Dirichlet prior (initial bead counts) and observing categorical data (game outcomes), the posterior is also Dirichlet with updated parameters.\nIn MENACE’s context:\n\nThe initial bead counts \\(\\alpha\\) represent prior beliefs about which moves are good\nEach game outcome adds evidence: winning adds to the count for moves that led to victory\nWin/draw increments are additive evidence; loss decrements are a heuristic/forgetting step\nThe posterior \\(\\text{Dir}(\\alpha + n)\\) is the updated belief after incorporating this evidence\n\nThe key property is that updating only requires adding integers — no complex calculations needed. This is why Michie could implement Bayesian inference with physical beads: the mathematics naturally maps to counting operations.\nThis conjugacy is not a coincidence but reflects a deep connection between counting and probability. The Dirichlet distribution is the natural prior for categorical probabilities precisely because it makes updating trivial through simple addition."
  },
  {
    "objectID": "presentation.html#a3-posterior-predictive-probability-matching",
    "href": "presentation.html#a3-posterior-predictive-probability-matching",
    "title": "MENACE as a Bayesian Observer",
    "section": "A3: Posterior Predictive Probability Matching",
    "text": "A3: Posterior Predictive Probability Matching\nTheorem: Drawing beads = posterior predictive probability matching.\nProof: For a Dirichlet–categorical model,\n\\[P(a) = \\int \\theta_a \\cdot \\text{Dir}(\\theta; \\alpha) \\, d\\theta = \\frac{\\alpha_a}{\\sum_i \\alpha_i}\\]\nMENACE’s probability \\[P(a) = \\frac{\\text{# beads of color } a}{\\text{total beads}} = \\frac{\\alpha_a}{\\sum_i \\alpha_i}\\]\nIdentical ∎\n\n\\(P(a)\\): probability of action \\(a\\)   •   \\(\\theta_a\\): move probability for action \\(a\\)   •   \\(\\text{Dir}(\\theta; \\alpha)\\): Dirichlet distribution   •   \\(\\alpha_a\\): bead count for action \\(a\\)   •   \\(\\sum_i \\alpha_i\\): total beads\n\n\nThe probability of action \\(a\\) equals the integral of \\(\\theta_a\\) times the Dirichlet density, which equals \\(\\alpha_a\\) over the sum of all alphas.\nMENACE samples directly from the posterior predictive distribution (the Dirichlet mean). This is probability matching, not canonical Thompson sampling.\nThompson sampling would first sample \\(\\theta \\sim \\text{Dir}(\\alpha)\\) and then take \\(\\arg\\max_a \\theta_a\\). MENACE instead samples actions in proportion to \\(\\alpha/\\alpha_0\\), which naturally implements mixed strategies."
  },
  {
    "objectID": "presentation.html#a4-dirichletcategorical-mutual-information",
    "href": "presentation.html#a4-dirichletcategorical-mutual-information",
    "title": "MENACE as a Bayesian Observer",
    "section": "A4: Dirichlet–Categorical Mutual Information",
    "text": "A4: Dirichlet–Categorical Mutual Information\nEpistemic value in the thesis is the mutual information between observations and parameters.\n\\[I(o;\\theta) = H\\!\\left[\\text{Cat}\\!\\left(\\frac{\\alpha}{\\alpha_0}\\right)\\right] - \\left[\\psi(\\alpha_0 + 1) - \\sum_i \\frac{\\alpha_i}{\\alpha_0} \\psi(\\alpha_i + 1)\\right]\\]\nEquivalent form:\n\\[I(o;\\theta) = \\sum_i \\frac{\\alpha_i}{\\alpha_0}\\!\\left[\\psi(\\alpha_i{+}1)-\\psi(\\alpha_0{+}1)-\\ln\\!\\frac{\\alpha_i}{\\alpha_0}\\right] \\ge 0\\]\nAs total concentration \\(\\alpha_0\\) becomes large, \\(I(o;\\theta) \\to 0\\).\n\n\\(I(o;\\theta)\\): mutual information   •   \\(o\\): observation (action or outcome)   •   \\(\\theta \\sim \\text{Dir}(\\alpha)\\): probability vector   •   \\(\\alpha_i\\): concentration parameter for category \\(i\\)   •   \\(\\alpha_0 = \\sum_i \\alpha_i\\): total concentration   •   \\(\\psi\\): digamma function   •   \\(H\\): entropy\n\n\nThe mutual information between observations \\(o\\) and parameters \\(\\theta\\) equals the entropy of the categorical distribution minus the expected conditional entropy, expressed using digamma functions \\(\\psi\\).\nThis formula quantifies epistemic value — how much observing a sample from the categorical distribution reveals about the underlying parameters. In MENACE’s context, \\(o\\) can be either an action selection (at the matchbox level) or a game outcome (at the trajectory level), depending on what Dirichlet is being queried.\nThe first form decomposes mutual information as entropy minus conditional entropy. The term \\(H[\\text{Cat}(\\alpha/\\alpha_0)]\\) is the entropy of the posterior predictive categorical distribution (uncertainty about the next observation). The bracketed term is the expected entropy of the likelihood given the parameters.\nThe equivalent form makes the non-negativity explicit. Each term in the sum is non-negative because the digamma function satisfies \\(\\psi(x+1) - \\psi(y+1) \\geq \\ln(x/y)\\) when \\(x \\leq y\\).\nThe key insight: as concentration \\(\\alpha_0\\) grows, mutual information shrinks toward zero. With high concentration (high confidence), the probabilities are already well-estimated — observing another sample teaches little. With low concentration (high uncertainty), each observation is highly informative.\nThis explains why epistemic value matters early in learning but diminishes over time. An agent that weights epistemic value highly will explore more when uncertain, then naturally transition to exploitation as beliefs consolidate."
  },
  {
    "objectID": "presentation.html#a5-deriving-variational-free-energy",
    "href": "presentation.html#a5-deriving-variational-free-energy",
    "title": "MENACE as a Bayesian Observer",
    "section": "A5: Deriving Variational Free Energy",
    "text": "A5: Deriving Variational Free Energy\nGoal: Approximate intractable posterior \\(p(s|o)\\) with tractable \\(q(s|o)\\).\n\\[\n\\begin{align}\nD_{KL}[q(s|o) \\| p(s|o)] &= \\mathbb{E}_q\\left[\\ln q(s|o) - \\ln \\frac{p(o|s)p(s)}{p(o)}\\right] \\\\\n&= \\mathbb{E}_q[\\ln q(s|o) - \\ln p(o|s) - \\ln p(s)] + \\ln p(o) \\\\\n&= \\underbrace{D_{KL}[q(s|o) \\| p(s)] - \\mathbb{E}_q[\\ln p(o|s)]}_{F \\;=\\; \\text{Free Energy}} + \\ln p(o)\n\\end{align}\n\\]\nSince \\(D_{KL} \\geq 0\\): \\(\\quad F \\geq -\\ln p(o) = \\text{Surprise}\\)\nKey insight: Minimizing \\(F\\) simultaneously approximates the posterior and bounds surprise.\n\n\\(q(s|o)\\): approximate posterior   •   \\(p(s|o)\\): true posterior   •   \\(p(o)\\): evidence/marginal likelihood   •   \\(F\\): variational free energy\n\n\nThis derivation shows how variational free energy arises from trying to minimize KL divergence between an approximate posterior and the true posterior.\nStarting with the KL divergence definition and applying Bayes’ rule to expand \\(p(s|o)\\), we can rearrange terms to isolate what we call free energy.\nThe key insight is that since KL divergence is always non-negative, free energy is always an upper bound on surprise (negative log evidence). Minimizing free energy therefore accomplishes two things simultaneously: it makes the approximate posterior closer to the true posterior, and it minimizes surprise about observations.\nThis is the mathematical foundation for why Active Inference agents minimize free energy — it is a tractable proxy for minimizing surprise while performing approximate Bayesian inference."
  },
  {
    "objectID": "presentation.html#a6-three-equivalent-forms-of-free-energy",
    "href": "presentation.html#a6-three-equivalent-forms-of-free-energy",
    "title": "MENACE as a Bayesian Observer",
    "section": "A6: Three Equivalent Forms of Free Energy",
    "text": "A6: Three Equivalent Forms of Free Energy\nEnergy − Entropy: Fit data while maintaining uncertainty \\[F = \\underbrace{-\\mathbb{E}_q[\\ln p(o,s)]}_{\\text{Energy}} + \\underbrace{H[q(s \\mid o)]}_{\\text{Entropy}}\\]\nComplexity − Accuracy: Simple models that explain data well \\[F = \\underbrace{D_{KL}[q \\parallel p(s)]}_{\\text{Complexity}} - \\underbrace{\\mathbb{E}_q[\\ln p(o \\mid s)]}_{\\text{Accuracy}}\\]\nDivergence + Surprise: Approximate inference while minimizing surprise \\[F = \\underbrace{D_{KL}[q \\parallel p(s \\mid o)]}_{\\text{Divergence}} + \\underbrace{(-\\ln p(o))}_{\\text{Surprise}}\\]\n\nThese three formulations of variational free energy are mathematically equivalent but emphasize different aspects of the optimization.\nThe Energy-Entropy form comes from statistical mechanics and shows the balance between fitting observations (low energy) and maintaining appropriate uncertainty (high entropy).\nThe Complexity-Accuracy form is common in machine learning. Complexity penalizes posteriors that deviate from the prior, while accuracy rewards explaining the data. This is the form used in variational autoencoders.\nThe Divergence-Surprise form most directly connects to Active Inference. It shows that free energy equals the gap between approximate and true posteriors plus surprise. When the approximate posterior equals the true posterior, free energy equals surprise exactly.\nFor MENACE and Active Inference, the Complexity-Accuracy form is most relevant: risk corresponds to the accuracy term (achieving preferred outcomes), while the Dirichlet prior provides implicit complexity regularization."
  },
  {
    "objectID": "slides/appendix-01-donald-michie.html",
    "href": "slides/appendix-01-donald-michie.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "The Pioneer Behind MENACE\n\nCodebreaker at Bletchley Park during WWII\nWorked alongside Alan Turing on Colossus\nMA, DPhil, DSc in biological sciences (Oxford)\nProfessor of Machine Intelligence, Edinburgh\n\nKey insight: “Programming human intelligence into machines” — inspired by wartime cryptanalysis.\n\nContributions to AI\n\nFounded Edinburgh’s Machine Intelligence unit\nEditor-in-Chief, “Machine Intelligence” series\nDeveloped machine learning into “industrial-strength tool”\n1996 Feigenbaum Medal for ML applications\n\nMENACE (1961): Physical demonstration that learning could be mechanized.\n\n\n\nDonald Michie was a polymath who bridged biology, computing, and artificial intelligence. His wartime experience at Bletchley Park, working alongside Alan Turing on Colossus, convinced him that machines could exhibit intelligent behavior through systematic processes.\nMENACE — the Machine Educable Noughts And Crosses Engine — was Michie’s 1961 demonstration that learning could emerge from simple mechanical operations: matchboxes, colored beads, and a reinforcement rule."
  },
  {
    "objectID": "slides/appendix-01-donald-michie.html#a1-donald-michie-1923-2007",
    "href": "slides/appendix-01-donald-michie.html#a1-donald-michie-1923-2007",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "The Pioneer Behind MENACE\n\nCodebreaker at Bletchley Park during WWII\nWorked alongside Alan Turing on Colossus\nMA, DPhil, DSc in biological sciences (Oxford)\nProfessor of Machine Intelligence, Edinburgh\n\nKey insight: “Programming human intelligence into machines” — inspired by wartime cryptanalysis.\n\nContributions to AI\n\nFounded Edinburgh’s Machine Intelligence unit\nEditor-in-Chief, “Machine Intelligence” series\nDeveloped machine learning into “industrial-strength tool”\n1996 Feigenbaum Medal for ML applications\n\nMENACE (1961): Physical demonstration that learning could be mechanized.\n\n\n\nDonald Michie was a polymath who bridged biology, computing, and artificial intelligence. His wartime experience at Bletchley Park, working alongside Alan Turing on Colossus, convinced him that machines could exhibit intelligent behavior through systematic processes.\nMENACE — the Machine Educable Noughts And Crosses Engine — was Michie’s 1961 demonstration that learning could emerge from simple mechanical operations: matchboxes, colored beads, and a reinforcement rule."
  },
  {
    "objectID": "slides/04-menace-as-fep.html",
    "href": "slides/04-menace-as-fep.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "What MENACE “believes”\n\\[p(o, s_{0:T}, a_{0:T}) = p(o|s_T) \\prod_{t} p(s_{t+1}|s_t, a_t) p(a_t|s_t)\\]\nwhere:\n\n\\(p(o|s_T)\\): Outcome model — final position determines outcome (deterministic)\n\\(p(s_{t+1}|s_t, a_t)\\): Transition model — game rules (fixed)\n\\(p(a_t|s_t)\\): Policy to learn (the beads)\n\nAll of MENACE’s learning focuses on optimizing the policy component.\n\n\\(o\\): outcome   •   \\(s_t\\): state at time \\(t\\)   •   \\(a_t\\): action at time \\(t\\)   •   \\(T\\): terminal time   •   \\(p(o|s_T)\\): outcome model   •   \\(p(s_{t+1}|s_t,a_t)\\): transition model   •   \\(p(a_t|s_t)\\): policy\n\n\nTo apply Active Inference to MENACE, we must specify its implicit generative model — the probabilistic model of how games unfold. This factorization has three components.\nFirst, the outcome model \\(p(o|s_T)\\): terminal board positions deterministically map to outcomes. MENACE encodes this implicitly — reinforcement is applied based on game results, so the system “knows” which positions are winning, losing, or drawing.\nSecond, the transition model \\(p(s_{t+1}|s_t, a_t)\\): the rules of Tic-Tac-Toe. These are fixed and encoded in how the human operator uses the system — the operator opens the matchbox corresponding to the current board state, so transitions are handled externally.\nThird, the policy \\(p(a_t|s_t)\\): this is what the beads represent. Each matchbox encodes MENACE’s beliefs about which moves are preferable in that position.\nAll of MENACE’s learning focuses on the policy component. The game rules and outcome mappings are fixed. Only the action preferences change.\nThis factorization is the same structure used by modern reinforcement learning algorithms, but implemented in physical hardware."
  },
  {
    "objectID": "slides/04-menace-as-fep.html#menaces-implicit-generative-model",
    "href": "slides/04-menace-as-fep.html#menaces-implicit-generative-model",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "What MENACE “believes”\n\\[p(o, s_{0:T}, a_{0:T}) = p(o|s_T) \\prod_{t} p(s_{t+1}|s_t, a_t) p(a_t|s_t)\\]\nwhere:\n\n\\(p(o|s_T)\\): Outcome model — final position determines outcome (deterministic)\n\\(p(s_{t+1}|s_t, a_t)\\): Transition model — game rules (fixed)\n\\(p(a_t|s_t)\\): Policy to learn (the beads)\n\nAll of MENACE’s learning focuses on optimizing the policy component.\n\n\\(o\\): outcome   •   \\(s_t\\): state at time \\(t\\)   •   \\(a_t\\): action at time \\(t\\)   •   \\(T\\): terminal time   •   \\(p(o|s_T)\\): outcome model   •   \\(p(s_{t+1}|s_t,a_t)\\): transition model   •   \\(p(a_t|s_t)\\): policy\n\n\nTo apply Active Inference to MENACE, we must specify its implicit generative model — the probabilistic model of how games unfold. This factorization has three components.\nFirst, the outcome model \\(p(o|s_T)\\): terminal board positions deterministically map to outcomes. MENACE encodes this implicitly — reinforcement is applied based on game results, so the system “knows” which positions are winning, losing, or drawing.\nSecond, the transition model \\(p(s_{t+1}|s_t, a_t)\\): the rules of Tic-Tac-Toe. These are fixed and encoded in how the human operator uses the system — the operator opens the matchbox corresponding to the current board state, so transitions are handled externally.\nThird, the policy \\(p(a_t|s_t)\\): this is what the beads represent. Each matchbox encodes MENACE’s beliefs about which moves are preferable in that position.\nAll of MENACE’s learning focuses on the policy component. The game rules and outcome mappings are fixed. Only the action preferences change.\nThis factorization is the same structure used by modern reinforcement learning algorithms, but implemented in physical hardware."
  },
  {
    "objectID": "slides/04-menace-as-fep.html#preference-weighted-policy-shaping",
    "href": "slides/04-menace-as-fep.html#preference-weighted-policy-shaping",
    "title": "MENACE as a Bayesian Observer",
    "section": "Preference-Weighted Policy Shaping",
    "text": "Preference-Weighted Policy Shaping\nMENACE corresponds to the instrumental objective\n\\[G_\\lambda(\\pi) = \\mathrm{Risk}(\\pi) - \\lambda I(o;\\theta), \\quad \\lambda = 0\\]\n\nPreferences are encoded by \\(U(o) \\in \\{+3,+1,-1\\}\\)\nBead updates shift Dirichlet counts toward preferred outcomes\nNegative updates are heuristic penalties; restocking keeps \\(\\alpha_{s,a} &gt; 0\\)\n\nThe update is directionally aligned with reducing instrumental risk, not an exact gradient step.\nThesis contribution: This model is instantiated for MENACE and Tic-Tac-Toe, with explicit computation of all quantities.\n\n\\(G_\\lambda(\\pi)\\): expected free energy   •   \\(\\pi\\): policy   •   \\(o\\): outcome   •   \\(\\theta\\): model parameters (bead proportions)   •   \\(I(o;\\theta)\\): information gain about \\(\\theta\\) from \\(o\\)   •   \\(U(o)\\): utility   •   \\(\\alpha_{s,a}\\): bead count\n\n\nReading the equation: “The expected free energy G sub lambda for policy pi equals the risk under that policy minus lambda times information gain, where lambda equals zero — so only the risk term remains.”\nThis slide states the central theoretical claim. MENACE corresponds to the instrumental special case of expected free energy minimization — that is, \\(\\lambda\\) equals zero, with no explicit epistemic drive.\nThe preferences encoded by the reinforcement schedule (+3/+1/-1) map to the prior preference distribution \\(p(o|C)\\) in the Active Inference formulation. Bead updates shift Dirichlet parameters toward outcomes with higher utility, which is directionally aligned with reducing the risk term — the KL divergence between predicted and preferred outcomes.\nThree important caveats qualify this claim.\nFirst, the update is directionally aligned with reducing risk, not an exact gradient step of a variational objective.\nSecond, loss updates (removing beads) are heuristic rather than conjugate Bayesian updates.\nThird, restocking — adding beads back when matchboxes empty — is a practical mechanism without a clean theoretical interpretation.\nDespite these caveats, my thesis contribution is to make this correspondence concrete: I instantiate this Active Inference model for MENACE and Tic-Tac-Toe, computing all quantities explicitly. This allows empirical testing of whether the theoretical correspondence holds in practice."
  },
  {
    "objectID": "slides/appendix-02-mathematical-details.html",
    "href": "slides/appendix-02-mathematical-details.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Theorem: If \\(\\theta \\sim \\text{Dir}(\\alpha)\\) and \\(n\\) outcomes are observed, then: \\[\\theta|\\text{data} \\sim \\text{Dir}(\\alpha + n)\\]\nProof:\n\\[\n\\begin{align}\np(\\theta|\\text{data}) &\\propto p(\\text{data}|\\theta)p(\\theta) \\\\\n&= \\prod_{i=1}^{k} \\theta_i^{n_i} \\cdot \\frac{1}{B(\\alpha)} \\prod_{i=1}^{k} \\theta_i^{\\alpha_i-1} \\\\\n&\\propto \\prod_{i=1}^{k} \\theta_i^{\\alpha_i + n_i - 1}\n\\end{align}\n\\]\nThis is the kernel of \\(\\text{Dir}(\\alpha + n)\\) ∎\n\nIf \\(\\theta\\) is drawn from a Dirichlet with parameters \\(\\alpha\\), and \\(n\\) outcomes are observed, then \\(\\theta\\) given data is Dirichlet with parameters \\(\\alpha\\) plus \\(n\\).\nThis proof shows why MENACE’s bead-updating mechanism is mathematically elegant.\nThe Dirichlet-categorical conjugacy means that starting with a Dirichlet prior (initial bead counts) and observing categorical data (game outcomes), the posterior is also Dirichlet with updated parameters.\nIn MENACE’s context:\n\nThe initial bead counts \\(\\alpha\\) represent prior beliefs about which moves are good\nEach game outcome adds evidence: winning adds to the count for moves that led to victory\nWin/draw increments are additive evidence; loss decrements are a heuristic/forgetting step\nThe posterior \\(\\text{Dir}(\\alpha + n)\\) is the updated belief after incorporating this evidence\n\nThe key property is that updating only requires adding integers — no complex calculations needed. This is why Michie could implement Bayesian inference with physical beads: the mathematics naturally maps to counting operations.\nThis conjugacy is not a coincidence but reflects a deep connection between counting and probability. The Dirichlet distribution is the natural prior for categorical probabilities precisely because it makes updating trivial through simple addition."
  },
  {
    "objectID": "slides/appendix-02-mathematical-details.html#a2-dirichlet-categorical-conjugacy-proof",
    "href": "slides/appendix-02-mathematical-details.html#a2-dirichlet-categorical-conjugacy-proof",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Theorem: If \\(\\theta \\sim \\text{Dir}(\\alpha)\\) and \\(n\\) outcomes are observed, then: \\[\\theta|\\text{data} \\sim \\text{Dir}(\\alpha + n)\\]\nProof:\n\\[\n\\begin{align}\np(\\theta|\\text{data}) &\\propto p(\\text{data}|\\theta)p(\\theta) \\\\\n&= \\prod_{i=1}^{k} \\theta_i^{n_i} \\cdot \\frac{1}{B(\\alpha)} \\prod_{i=1}^{k} \\theta_i^{\\alpha_i-1} \\\\\n&\\propto \\prod_{i=1}^{k} \\theta_i^{\\alpha_i + n_i - 1}\n\\end{align}\n\\]\nThis is the kernel of \\(\\text{Dir}(\\alpha + n)\\) ∎\n\nIf \\(\\theta\\) is drawn from a Dirichlet with parameters \\(\\alpha\\), and \\(n\\) outcomes are observed, then \\(\\theta\\) given data is Dirichlet with parameters \\(\\alpha\\) plus \\(n\\).\nThis proof shows why MENACE’s bead-updating mechanism is mathematically elegant.\nThe Dirichlet-categorical conjugacy means that starting with a Dirichlet prior (initial bead counts) and observing categorical data (game outcomes), the posterior is also Dirichlet with updated parameters.\nIn MENACE’s context:\n\nThe initial bead counts \\(\\alpha\\) represent prior beliefs about which moves are good\nEach game outcome adds evidence: winning adds to the count for moves that led to victory\nWin/draw increments are additive evidence; loss decrements are a heuristic/forgetting step\nThe posterior \\(\\text{Dir}(\\alpha + n)\\) is the updated belief after incorporating this evidence\n\nThe key property is that updating only requires adding integers — no complex calculations needed. This is why Michie could implement Bayesian inference with physical beads: the mathematics naturally maps to counting operations.\nThis conjugacy is not a coincidence but reflects a deep connection between counting and probability. The Dirichlet distribution is the natural prior for categorical probabilities precisely because it makes updating trivial through simple addition."
  },
  {
    "objectID": "slides/appendix-02-mathematical-details.html#a3-posterior-predictive-probability-matching",
    "href": "slides/appendix-02-mathematical-details.html#a3-posterior-predictive-probability-matching",
    "title": "MENACE as a Bayesian Observer",
    "section": "A3: Posterior Predictive Probability Matching",
    "text": "A3: Posterior Predictive Probability Matching\nTheorem: Drawing beads = posterior predictive probability matching.\nProof: For a Dirichlet–categorical model,\n\\[P(a) = \\int \\theta_a \\cdot \\text{Dir}(\\theta; \\alpha) \\, d\\theta = \\frac{\\alpha_a}{\\sum_i \\alpha_i}\\]\nMENACE’s probability \\[P(a) = \\frac{\\text{# beads of color } a}{\\text{total beads}} = \\frac{\\alpha_a}{\\sum_i \\alpha_i}\\]\nIdentical ∎\n\n\\(P(a)\\): probability of action \\(a\\)   •   \\(\\theta_a\\): move probability for action \\(a\\)   •   \\(\\text{Dir}(\\theta; \\alpha)\\): Dirichlet distribution   •   \\(\\alpha_a\\): bead count for action \\(a\\)   •   \\(\\sum_i \\alpha_i\\): total beads\n\n\nThe probability of action \\(a\\) equals the integral of \\(\\theta_a\\) times the Dirichlet density, which equals \\(\\alpha_a\\) over the sum of all alphas.\nMENACE samples directly from the posterior predictive distribution (the Dirichlet mean). This is probability matching, not canonical Thompson sampling.\nThompson sampling would first sample \\(\\theta \\sim \\text{Dir}(\\alpha)\\) and then take \\(\\arg\\max_a \\theta_a\\). MENACE instead samples actions in proportion to \\(\\alpha/\\alpha_0\\), which naturally implements mixed strategies."
  },
  {
    "objectID": "slides/appendix-02-mathematical-details.html#a4-dirichletcategorical-mutual-information",
    "href": "slides/appendix-02-mathematical-details.html#a4-dirichletcategorical-mutual-information",
    "title": "MENACE as a Bayesian Observer",
    "section": "A4: Dirichlet–Categorical Mutual Information",
    "text": "A4: Dirichlet–Categorical Mutual Information\nEpistemic value in the thesis is the mutual information between observations and parameters.\n\\[I(o;\\theta) = H\\!\\left[\\text{Cat}\\!\\left(\\frac{\\alpha}{\\alpha_0}\\right)\\right] - \\left[\\psi(\\alpha_0 + 1) - \\sum_i \\frac{\\alpha_i}{\\alpha_0} \\psi(\\alpha_i + 1)\\right]\\]\nEquivalent form:\n\\[I(o;\\theta) = \\sum_i \\frac{\\alpha_i}{\\alpha_0}\\!\\left[\\psi(\\alpha_i{+}1)-\\psi(\\alpha_0{+}1)-\\ln\\!\\frac{\\alpha_i}{\\alpha_0}\\right] \\ge 0\\]\nAs total concentration \\(\\alpha_0\\) becomes large, \\(I(o;\\theta) \\to 0\\).\n\n\\(I(o;\\theta)\\): mutual information   •   \\(o\\): observation (action or outcome)   •   \\(\\theta \\sim \\text{Dir}(\\alpha)\\): probability vector   •   \\(\\alpha_i\\): concentration parameter for category \\(i\\)   •   \\(\\alpha_0 = \\sum_i \\alpha_i\\): total concentration   •   \\(\\psi\\): digamma function   •   \\(H\\): entropy\n\n\nThe mutual information between observations \\(o\\) and parameters \\(\\theta\\) equals the entropy of the categorical distribution minus the expected conditional entropy, expressed using digamma functions \\(\\psi\\).\nThis formula quantifies epistemic value — how much observing a sample from the categorical distribution reveals about the underlying parameters. In MENACE’s context, \\(o\\) can be either an action selection (at the matchbox level) or a game outcome (at the trajectory level), depending on what Dirichlet is being queried.\nThe first form decomposes mutual information as entropy minus conditional entropy. The term \\(H[\\text{Cat}(\\alpha/\\alpha_0)]\\) is the entropy of the posterior predictive categorical distribution (uncertainty about the next observation). The bracketed term is the expected entropy of the likelihood given the parameters.\nThe equivalent form makes the non-negativity explicit. Each term in the sum is non-negative because the digamma function satisfies \\(\\psi(x+1) - \\psi(y+1) \\geq \\ln(x/y)\\) when \\(x \\leq y\\).\nThe key insight: as concentration \\(\\alpha_0\\) grows, mutual information shrinks toward zero. With high concentration (high confidence), the probabilities are already well-estimated — observing another sample teaches little. With low concentration (high uncertainty), each observation is highly informative.\nThis explains why epistemic value matters early in learning but diminishes over time. An agent that weights epistemic value highly will explore more when uncertain, then naturally transition to exploitation as beliefs consolidate."
  },
  {
    "objectID": "slides/appendix-02-mathematical-details.html#a5-deriving-variational-free-energy",
    "href": "slides/appendix-02-mathematical-details.html#a5-deriving-variational-free-energy",
    "title": "MENACE as a Bayesian Observer",
    "section": "A5: Deriving Variational Free Energy",
    "text": "A5: Deriving Variational Free Energy\nGoal: Approximate intractable posterior \\(p(s|o)\\) with tractable \\(q(s|o)\\).\n\\[\n\\begin{align}\nD_{KL}[q(s|o) \\| p(s|o)] &= \\mathbb{E}_q\\left[\\ln q(s|o) - \\ln \\frac{p(o|s)p(s)}{p(o)}\\right] \\\\\n&= \\mathbb{E}_q[\\ln q(s|o) - \\ln p(o|s) - \\ln p(s)] + \\ln p(o) \\\\\n&= \\underbrace{D_{KL}[q(s|o) \\| p(s)] - \\mathbb{E}_q[\\ln p(o|s)]}_{F \\;=\\; \\text{Free Energy}} + \\ln p(o)\n\\end{align}\n\\]\nSince \\(D_{KL} \\geq 0\\): \\(\\quad F \\geq -\\ln p(o) = \\text{Surprise}\\)\nKey insight: Minimizing \\(F\\) simultaneously approximates the posterior and bounds surprise.\n\n\\(q(s|o)\\): approximate posterior   •   \\(p(s|o)\\): true posterior   •   \\(p(o)\\): evidence/marginal likelihood   •   \\(F\\): variational free energy\n\n\nThis derivation shows how variational free energy arises from trying to minimize KL divergence between an approximate posterior and the true posterior.\nStarting with the KL divergence definition and applying Bayes’ rule to expand \\(p(s|o)\\), we can rearrange terms to isolate what we call free energy.\nThe key insight is that since KL divergence is always non-negative, free energy is always an upper bound on surprise (negative log evidence). Minimizing free energy therefore accomplishes two things simultaneously: it makes the approximate posterior closer to the true posterior, and it minimizes surprise about observations.\nThis is the mathematical foundation for why Active Inference agents minimize free energy — it is a tractable proxy for minimizing surprise while performing approximate Bayesian inference."
  },
  {
    "objectID": "slides/appendix-02-mathematical-details.html#a6-three-equivalent-forms-of-free-energy",
    "href": "slides/appendix-02-mathematical-details.html#a6-three-equivalent-forms-of-free-energy",
    "title": "MENACE as a Bayesian Observer",
    "section": "A6: Three Equivalent Forms of Free Energy",
    "text": "A6: Three Equivalent Forms of Free Energy\nEnergy − Entropy: Fit data while maintaining uncertainty \\[F = \\underbrace{-\\mathbb{E}_q[\\ln p(o,s)]}_{\\text{Energy}} + \\underbrace{H[q(s \\mid o)]}_{\\text{Entropy}}\\]\nComplexity − Accuracy: Simple models that explain data well \\[F = \\underbrace{D_{KL}[q \\parallel p(s)]}_{\\text{Complexity}} - \\underbrace{\\mathbb{E}_q[\\ln p(o \\mid s)]}_{\\text{Accuracy}}\\]\nDivergence + Surprise: Approximate inference while minimizing surprise \\[F = \\underbrace{D_{KL}[q \\parallel p(s \\mid o)]}_{\\text{Divergence}} + \\underbrace{(-\\ln p(o))}_{\\text{Surprise}}\\]\n\nThese three formulations of variational free energy are mathematically equivalent but emphasize different aspects of the optimization.\nThe Energy-Entropy form comes from statistical mechanics and shows the balance between fitting observations (low energy) and maintaining appropriate uncertainty (high entropy).\nThe Complexity-Accuracy form is common in machine learning. Complexity penalizes posteriors that deviate from the prior, while accuracy rewards explaining the data. This is the form used in variational autoencoders.\nThe Divergence-Surprise form most directly connects to Active Inference. It shows that free energy equals the gap between approximate and true posteriors plus surprise. When the approximate posterior equals the true posterior, free energy equals surprise exactly.\nFor MENACE and Active Inference, the Complexity-Accuracy form is most relevant: risk corresponds to the accuracy term (achieving preferred outcomes), while the Dirichlet prior provides implicit complexity regularization."
  },
  {
    "objectID": "slides/05-empirical-results.html",
    "href": "slides/05-empirical-results.html",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Agents Compared\n\nMENACE (Michie filter, box restock)\nInstrumental AIF (\\(\\lambda = 0\\))\nHybrid AIF (\\(\\lambda = 0.5\\))\nPure AIF (\\(\\lambda \\in \\{0, 0.25, 0.5\\}\\))\nQ-Learning / SARSA baselines\n\n\nProtocol\n\nTraining: 500 games (5,000 for RL)\nValidation: 100 games vs. optimal\nSeeds: 10 independent runs\nState filters: Michie (287 states)\nMENACE curriculum: mixed; AIF baseline: optimal-only\n\n\n\nThesis contribution: First systematic empirical comparison of MENACE against Active Inference variants.\n\nThe theoretical correspondence makes testable predictions. If MENACE corresponds to instrumental Active Inference with \\(\\lambda = 0\\), then an explicit Active Inference agent with \\(\\lambda = 0\\) should achieve similar performance.\nThe experiments compare five agent types:\n\nMENACE with Michie’s original 287-state filter and box-level restocking\nInstrumental Active Inference with \\(\\lambda = 0\\)\nHybrid Active Inference, which combines MENACE-style Dirichlet updates with EFE-based action selection\nPure Active Inference, which uses EFE for both learning and action selection without the Dirichlet machinery\nTabular Reinforcement Learning baselines: Q-learning and SARSA\n\nBoth Hybrid and Pure variants are tested with positive \\(\\lambda\\) values to add explicit epistemic drive. Tabular RL baselines serve as reference points from the reinforcement learning literature.\nSARSA (State-Action-Reward-State-Action) is on-policy temporal difference control: it updates Q-values using the action actually taken next, whereas Q-learning is off-policy and updates using the maximum over next actions.\nAll agents are trained for 500 games, except RL baselines which receive 5,000 games for asymptotic comparison. Post-training validation uses 100 games against optimal play. Results aggregate over 10 independent seeds.\nOne methodological note: MENACE uses a mixed curriculum — starting against random opponents and progressing to optimal — while the AIF baseline trains directly against optimal. This means training trajectories are not directly comparable, but post-training validation against the same opponent provides the fair comparison point."
  },
  {
    "objectID": "slides/05-empirical-results.html#experimental-setup",
    "href": "slides/05-empirical-results.html#experimental-setup",
    "title": "MENACE as a Bayesian Observer",
    "section": "",
    "text": "Agents Compared\n\nMENACE (Michie filter, box restock)\nInstrumental AIF (\\(\\lambda = 0\\))\nHybrid AIF (\\(\\lambda = 0.5\\))\nPure AIF (\\(\\lambda \\in \\{0, 0.25, 0.5\\}\\))\nQ-Learning / SARSA baselines\n\n\nProtocol\n\nTraining: 500 games (5,000 for RL)\nValidation: 100 games vs. optimal\nSeeds: 10 independent runs\nState filters: Michie (287 states)\nMENACE curriculum: mixed; AIF baseline: optimal-only\n\n\n\nThesis contribution: First systematic empirical comparison of MENACE against Active Inference variants.\n\nThe theoretical correspondence makes testable predictions. If MENACE corresponds to instrumental Active Inference with \\(\\lambda = 0\\), then an explicit Active Inference agent with \\(\\lambda = 0\\) should achieve similar performance.\nThe experiments compare five agent types:\n\nMENACE with Michie’s original 287-state filter and box-level restocking\nInstrumental Active Inference with \\(\\lambda = 0\\)\nHybrid Active Inference, which combines MENACE-style Dirichlet updates with EFE-based action selection\nPure Active Inference, which uses EFE for both learning and action selection without the Dirichlet machinery\nTabular Reinforcement Learning baselines: Q-learning and SARSA\n\nBoth Hybrid and Pure variants are tested with positive \\(\\lambda\\) values to add explicit epistemic drive. Tabular RL baselines serve as reference points from the reinforcement learning literature.\nSARSA (State-Action-Reward-State-Action) is on-policy temporal difference control: it updates Q-values using the action actually taken next, whereas Q-learning is off-policy and updates using the maximum over next actions.\nAll agents are trained for 500 games, except RL baselines which receive 5,000 games for asymptotic comparison. Post-training validation uses 100 games against optimal play. Results aggregate over 10 independent seeds.\nOne methodological note: MENACE uses a mixed curriculum — starting against random opponents and progressing to optimal — while the AIF baseline trains directly against optimal. This means training trajectories are not directly comparable, but post-training validation against the same opponent provides the fair comparison point."
  },
  {
    "objectID": "slides/05-empirical-results.html#key-finding-1-instrumental-equivalence",
    "href": "slides/05-empirical-results.html#key-finding-1-instrumental-equivalence",
    "title": "MENACE as a Bayesian Observer",
    "section": "Key Finding 1: Instrumental Equivalence",
    "text": "Key Finding 1: Instrumental Equivalence\n\n\n\nAlgorithm\nDraw Rate (%)\nLoss Rate (%)\n\n\n\n\nMENACE (box restock)\n\\(84.5 \\pm 8.1\\)\n\\(15.5 \\pm 8.1\\)\n\n\nInstrumental AIF (\\(\\lambda=0\\))\n\\(88.1 \\pm 3.9\\)\n\\(11.9 \\pm 3.9\\)\n\n\n\n\nWhen \\(\\lambda = 0\\), the EFE-based policy reduces to a purely instrumental objective, closely matching MENACE’s pseudo-count reinforcement\nHowever, MENACE has implicit exploration: low concentration → high variance → naturally exploratory early behavior\n\nResult: MENACE ≈ Instrumental Active Inference.\n\nThe first key finding tests the central theoretical claim: does MENACE correspond to instrumental Active Inference?\nMENACE achieves 84.5% draw rate; Instrumental AIF with \\(\\lambda = 0\\) achieves 88.1%. Overlapping confidence intervals support the correspondence — both optimize approximately the same objective.\nResidual differences come from implementation details. Note that MENACE has implicit exploration via the Dirichlet: low concentration means high variance, which diminishes as beliefs consolidate."
  },
  {
    "objectID": "slides/05-empirical-results.html#key-finding-2-the-value-of-information",
    "href": "slides/05-empirical-results.html#key-finding-2-the-value-of-information",
    "title": "MENACE as a Bayesian Observer",
    "section": "Key Finding 2: The Value of Information",
    "text": "Key Finding 2: The Value of Information\n\n\n\nAlgorithm\nDraw Rate (%)\n\n\n\n\nPure AIF (\\(\\lambda = 0.0\\))\n\\(79.7 \\pm 6.5\\)\n\n\nPure AIF (\\(\\lambda = 0.25\\))\n\\(79.1 \\pm 4.8\\)\n\n\nPure AIF (\\(\\lambda = 0.5\\))\n\\(77.0 \\pm 3.7\\)\n\n\n\nParadox: Epistemic variants (\\(\\lambda &gt; 0\\)) do NOT outperform instrumental baseline (\\(\\lambda=0\\)) within 500 games.\nResolution: This reflects Michie’s trade-off in practice: “acquisition of information for future use at the expense of present expected gain”.\n\nThe second finding addresses Michie’s original question: what is the cost of information?\nHigher \\(\\lambda\\) means lower short-term performance: 79.7% at \\(\\lambda = 0\\) down to 77.0% at \\(\\lambda = 0.5\\). This confirms Michie’s prediction — information has value for future decisions, but acquiring it costs immediate performance.\nThis quantifies the trade-off: explicit epistemic value has a measurable short-horizon cost."
  },
  {
    "objectID": "slides/05-empirical-results.html#key-finding-3-robustness-vs.-specialization",
    "href": "slides/05-empirical-results.html#key-finding-3-robustness-vs.-specialization",
    "title": "MENACE as a Bayesian Observer",
    "section": "Key Finding 3: Robustness vs. Specialization",
    "text": "Key Finding 3: Robustness vs. Specialization\n\n\n\nAlgorithm\nTraining\nDraw Rate (%)\n\n\n\n\nQ-learning\nrandom\n\\(98.0 \\pm \\phantom{0}1.2\\)\n\n\nQ-learning\ndefensive\n\\(10.2 \\pm 30.9\\)\n\n\nSARSA\nrandom\n\\(97.9 \\pm \\phantom{0}1.9\\)\n\n\nSARSA\ndefensive\n\\(20.5 \\pm 40.3\\)\n\n\n\n\nBox-level restocking preserves full support\nProvides implicit “insurance” against distributional shift\nQ-learning can drive Q-values to extremes, zeroing out actions\nMENACE achieves competitive performance with 10× fewer games\n\nDirichlet advantage: MENACE keeps all action probabilities strictly positive.\n\nThe third finding compares MENACE’s robustness against tabular RL baselines.\nRL achieves 98% when training and evaluation match, but collapses under distribution shift — Q-learning trained on defensive drops to 10% against optimal. MENACE’s restocking ensures all actions keep positive probability, providing implicit regularization.\nAlso note sample efficiency: MENACE achieves competitive performance with 500 games versus 5,000 for RL."
  }
]